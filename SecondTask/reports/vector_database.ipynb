{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c7ec89703f0e43d7a910424a739a6878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90e480dbb895491ca6e599a1ca53809d",
              "IPY_MODEL_8aa12787fe794eeb9af8603c5f5a5616",
              "IPY_MODEL_49aa9ba6499f4af290645fe9ba872a2e"
            ],
            "layout": "IPY_MODEL_ff1546912dc1455c9532fa4c0e766efc"
          }
        },
        "90e480dbb895491ca6e599a1ca53809d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c4ccaaf9d37463eb9653c315fbc4799",
            "placeholder": "​",
            "style": "IPY_MODEL_eec336f6907641e498bdf8b7c22bf979",
            "value": "Batches: 100%"
          }
        },
        "8aa12787fe794eeb9af8603c5f5a5616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_879b344048f5468893505371d8e8deef",
            "max": 157,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_820064b1998f460491e623598404cd99",
            "value": 157
          }
        },
        "49aa9ba6499f4af290645fe9ba872a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d982a43e38849f0ade384ce7efbd1e0",
            "placeholder": "​",
            "style": "IPY_MODEL_2f71c49524e9418fbfdcde51bc422c37",
            "value": " 157/157 [01:31&lt;00:00,  3.30it/s]"
          }
        },
        "ff1546912dc1455c9532fa4c0e766efc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c4ccaaf9d37463eb9653c315fbc4799": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eec336f6907641e498bdf8b7c22bf979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "879b344048f5468893505371d8e8deef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "820064b1998f460491e623598404cd99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d982a43e38849f0ade384ce7efbd1e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f71c49524e9418fbfdcde51bc422c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Vector Database Indexing Algorithms Performance Testing\n",
        "# Google Colab Notebook for comparing vector DB efficiency and accuracy\n",
        "\n",
        "\"\"\"\n",
        "This notebook tests multiple vector database indexing algorithms including:\n",
        "1. FAISS (Multiple index types: Flat, IVF, HNSW, PQ)\n",
        "2. ChromaDB (with different configurations)\n",
        "3. Qdrant (if available)\n",
        "4. Annoy (Approximate Nearest Neighbors)\n",
        "5. ScaNN (Google's Scalable Nearest Neighbors)\n",
        "6. NMSLIB (Non-Metric Space Library)\n",
        "\n",
        "Performance metrics evaluated:\n",
        "- Search accuracy (recall@k)\n",
        "- Query latency\n",
        "- Index build time\n",
        "- Memory usage\n",
        "- Throughput (queries per second)\n",
        "\"\"\"\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 2: PACKAGE INSTALLATION & IMPORTS\n",
        "# ==========================================\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 1: INSTALLATION & SETUP INSTRUCTIONS\n",
        "# ==========================================\n",
        "\n",
        "print(\"🔧 VECTOR DATABASE SETUP INSTRUCTIONS\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "print(\"📦 This notebook tests multiple vector databases with different configurations:\")\n",
        "print()\n",
        "print(\"✅ GUARANTEED TO WORK (No additional configuration needed):\")\n",
        "print(\"  • FAISS (4 variants) - Facebook's similarity search library\")\n",
        "print(\"  • ChromaDB (3 configurations) - Modern vector database\")\n",
        "print()\n",
        "print(\"⚙️  ADDITIONAL DATABASES (May have compatibility issues in Colab):\")\n",
        "print(\"  • Annoy - Spotify's approximate nearest neighbors\")\n",
        "print(\"  • NMSLIB - Non-metric space library (often fails in Colab)\")\n",
        "print(\"  • ScaNN - Google's scalable nearest neighbors (TensorFlow dependency issues)\")\n",
        "print(\"  • Qdrant - Production vector database\")\n",
        "print()\n",
        "print(\"🔧 OPTIONAL SETUP for enhanced testing:\")\n",
        "print()\n",
        "print(\"🐳 QDRANT LOCAL SERVER (for production-like testing):\")\n",
        "print(\"  1. Install Docker\")\n",
        "print(\"  2. Run: docker run -p 6333:6333 qdrant/qdrant\")\n",
        "print(\"  3. The notebook will automatically detect and use it\")\n",
        "print()\n",
        "print(\"☁️  QDRANT CLOUD (for cloud testing):\")\n",
        "print(\"  1. Sign up at https://cloud.qdrant.io/\")\n",
        "print(\"  2. Create a cluster\")\n",
        "print(\"  3. Uncomment cloud configuration in SECTION 11.5\")\n",
        "print(\"  4. Add your cluster URL and API key\")\n",
        "print()\n",
        "print(\"⚠️  COMMON ISSUES:\")\n",
        "print(\"  • NMSLIB: Often fails to compile in Colab environments\")\n",
        "print(\"  • ScaNN: May have TensorFlow compatibility issues\")\n",
        "print(\"  • These failures are normal and won't affect the core benchmarking\")\n",
        "print()\n",
        "print(\"🎯 The notebook will automatically test all available implementations\")\n",
        "print(\"   and provide comprehensive results even if some packages fail.\")\n",
        "print()\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 2: PACKAGE INSTALLATION & IMPORTS\n",
        "# ==========================================\n",
        "\n",
        "# Install core packages first\n",
        "!pip install faiss-cpu chromadb sentence-transformers numpy pandas matplotlib seaborn plotly\n",
        "!pip install scikit-learn tqdm psutil memory_profiler\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1N4y5uO-Dyp",
        "outputId": "167bedaa-16f8-41b0-db1b-0edeb8639063",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 VECTOR DATABASE SETUP INSTRUCTIONS\n",
            "============================================================\n",
            "\n",
            "📦 This notebook tests multiple vector databases with different configurations:\n",
            "\n",
            "✅ GUARANTEED TO WORK (No additional configuration needed):\n",
            "  • FAISS (4 variants) - Facebook's similarity search library\n",
            "  • ChromaDB (3 configurations) - Modern vector database\n",
            "\n",
            "⚙️  ADDITIONAL DATABASES (May have compatibility issues in Colab):\n",
            "  • Annoy - Spotify's approximate nearest neighbors\n",
            "  • NMSLIB - Non-metric space library (often fails in Colab)\n",
            "  • ScaNN - Google's scalable nearest neighbors (TensorFlow dependency issues)\n",
            "  • Qdrant - Production vector database\n",
            "\n",
            "🔧 OPTIONAL SETUP for enhanced testing:\n",
            "\n",
            "🐳 QDRANT LOCAL SERVER (for production-like testing):\n",
            "  1. Install Docker\n",
            "  2. Run: docker run -p 6333:6333 qdrant/qdrant\n",
            "  3. The notebook will automatically detect and use it\n",
            "\n",
            "☁️  QDRANT CLOUD (for cloud testing):\n",
            "  1. Sign up at https://cloud.qdrant.io/\n",
            "  2. Create a cluster\n",
            "  3. Uncomment cloud configuration in SECTION 11.5\n",
            "  4. Add your cluster URL and API key\n",
            "\n",
            "⚠️  COMMON ISSUES:\n",
            "  • NMSLIB: Often fails to compile in Colab environments\n",
            "  • ScaNN: May have TensorFlow compatibility issues\n",
            "  • These failures are normal and won't affect the core benchmarking\n",
            "\n",
            "🎯 The notebook will automatically test all available implementations\n",
            "   and provide comprehensive results even if some packages fail.\n",
            "\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.7)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.17.4)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.9)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (5.9.5)\n",
            "Requirement already satisfied: memory_profiler in /usr/local/lib/python3.12/dist-packages (0.61.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c7ec89703f0e43d7a910424a739a6878",
            "90e480dbb895491ca6e599a1ca53809d",
            "8aa12787fe794eeb9af8603c5f5a5616",
            "49aa9ba6499f4af290645fe9ba872a2e",
            "ff1546912dc1455c9532fa4c0e766efc",
            "8c4ccaaf9d37463eb9653c315fbc4799",
            "eec336f6907641e498bdf8b7c22bf979",
            "879b344048f5468893505371d8e8deef",
            "820064b1998f460491e623598404cd99",
            "6d982a43e38849f0ade384ce7efbd1e0",
            "2f71c49524e9418fbfdcde51bc422c37"
          ]
        },
        "id": "-KYMC0qE95Gd",
        "outputId": "e2a42133-ab93-4d19-8b29-08939389a6c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Installing vector database packages...\n",
            "Installing Annoy...\n",
            "✅ Annoy installed successfully\n",
            "Installing NMSLIB...\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for nmslib (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for nmslib\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (nmslib)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h✅ NMSLIB installed successfully\n",
            "Installing Qdrant...\n",
            "✅ Qdrant installed successfully\n",
            "Installing ScaNN...\n",
            "✅ ScaNN installed successfully\n",
            "✅ Annoy imported successfully\n",
            "⚠️ NMSLIB import failed: No module named 'nmslib'\n",
            "✅ Qdrant imported successfully\n",
            "⚠️ ScaNN import failed: /usr/local/lib/python3.12/dist-packages/scann/scann_ops/cc/_scann_ops.so: undefined symbol: _ZN4absl12lts_2025012716raw_log_internal21internal_log_functionB5cxx11E\n",
            "   This is often due to TensorFlow compatibility issues in Colab\n",
            "\n",
            "🔧 Performing final import verification...\n",
            "❌ NMSLIB: Compilation/import failed - will be skipped (common in Colab)\n",
            "❌ ScaNN: TensorFlow compatibility issue - will be skipped (common in Colab)\n",
            "\n",
            "📊 Final Vector Database Availability:\n",
            "  ✅ FAISS: Available (4 variants) - Core library\n",
            "  ✅ ChromaDB: Available (3 variants) - Core library\n",
            "  ✅ Annoy: Available\n",
            "  ❌ NMSLIB: Failed - Common in Colab environments\n",
            "  ✅ Qdrant: Available (4 variants)\n",
            "  ❌ ScaNN: Failed - TensorFlow compatibility\n",
            "\n",
            "🎯 Total vector database variants to test: 12\n",
            "\n",
            "🚀 Proceeding with 4 available vector database types...\n",
            "Please upload your StarTech dataset (CSV file):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c31eebc1-ae1b-4dd0-8ee3-73be43c393c5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c31eebc1-ae1b-4dd0-8ee3-73be43c393c5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving startech_fast_20250903_195133.csv to startech_fast_20250903_195133 (1).csv\n",
            "📁 Loaded file: startech_fast_20250903_195133 (1).csv\n",
            "📊 Dataset shape: (8462, 10)\n",
            "🏷️  Columns: ['name', 'price', 'brand', 'category', 'subcategory', 'availability', 'image_url', 'product_url', 'model', 'rating']\n",
            "📄 Created text representations from product data\n",
            "📉 Limiting dataset to 5000 documents for testing\n",
            "📝 Prepared 5000 documents for vector database testing\n",
            "\n",
            "============================================================\n",
            "🧠 GENERATING EMBEDDINGS\n",
            "============================================================\n",
            "📥 Loading embedding model...\n",
            "✅ Embedding model loaded successfully!\n",
            "🔄 Generating embeddings for all documents...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7ec89703f0e43d7a910424a739a6878"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Generated 5000 embeddings in 91.40 seconds\n",
            "📊 Embedding shape: (5000, 384)\n",
            "📏 Embedding dimension: 384\n",
            "🎯 Created 100 test queries for accuracy evaluation\n",
            "\n",
            "============================================================\n",
            "💻 SYSTEM INFORMATION\n",
            "============================================================\n",
            "🖥️  CPU Cores: 2\n",
            "💾 Total Memory: 12.7 GB\n",
            "💽 Available Memory: 9.8 GB\n",
            "🎯 Testing with k values: [1, 5, 10, 20]\n",
            "\n",
            "============================================================\n",
            "🎯 COMPUTING GROUND TRUTH\n",
            "============================================================\n",
            "🔄 Computing exact nearest neighbors for accuracy evaluation...\n",
            "✅ Ground truth computed in 0.02 seconds\n",
            "\n",
            "============================================================\n",
            "🚀 TESTING FAISS IMPLEMENTATIONS\n",
            "============================================================\n",
            "🔧 Building FAISS Flat index...\n",
            "✅ FAISS Flat built in 0.00s, Memory: 14.6MB\n",
            "🔧 Building FAISS IVF index (nlist=100)...\n",
            "✅ FAISS IVF built in 0.16s, Memory: 8.8MB\n",
            "🔧 Building FAISS HNSW index (M=16)...\n",
            "✅ FAISS HNSW built in 0.21s, Memory: 11.0MB\n",
            "🔧 Building FAISS PQ index (m=8)...\n",
            "✅ FAISS PQ built in 1.21s, Memory: 2.2MB\n",
            "\n",
            "============================================================\n",
            "🎨 TESTING CHROMADB\n",
            "============================================================\n",
            "🔧 Building ChromaDB default index...\n",
            "✅ ChromaDB Default built in 6.08s, Memory: 9.5MB\n",
            "🔧 Building ChromaDB optimized index...\n",
            "✅ ChromaDB Optimized built in 5.71s, Memory: 11.0MB\n",
            "🔧 Building ChromaDB fast index...\n",
            "✅ ChromaDB Fast built in 4.59s, Memory: 8.1MB\n",
            "\n",
            "============================================================\n",
            "🎪 TESTING ANNOY\n",
            "============================================================\n",
            "🔧 Building Annoy index (n_trees=10)...\n",
            "✅ Annoy built in 0.28s, Memory: 5.9MB\n",
            "\n",
            "============================================================\n",
            "⚠️  NMSLIB NOT AVAILABLE\n",
            "============================================================\n",
            "NMSLIB is not available. Skipping NMSLIB benchmarks.\n",
            "\n",
            "============================================================\n",
            "⚠️  SCANN NOT AVAILABLE\n",
            "============================================================\n",
            "ScaNN is not available. Skipping ScaNN benchmarks.\n",
            "\n",
            "============================================================\n",
            "🎯 TESTING QDRANT\n",
            "============================================================\n",
            "🔧 Qdrant Setup Options:\n",
            "  1. In-memory (for testing)\n",
            "  2. Local server (requires Docker)\n",
            "  3. Qdrant Cloud (requires API key)\n",
            "\n",
            "🔧 Building Qdrant in-memory index...\n",
            "✅ Qdrant Memory built in 2.35s, Memory: 10.3MB\n",
            "🔧 Building Qdrant optimized index...\n",
            "✅ Qdrant Optimized built in 2.34s, Memory: 11.7MB\n",
            "🔧 Attempting to connect to local Qdrant server...\n",
            "⚠️  Local Qdrant server not available: [Errno 99] Cannot assign requested address\n",
            "💡 To use local server, run: docker run -p 6333:6333 qdrant/qdrant\n",
            "🔧 Building Qdrant cloud index...\n",
            "\n",
            "============================================================\n",
            "🏃‍♂️ RUNNING COMPREHENSIVE BENCHMARKS\n",
            "============================================================\n",
            "🎯 Running benchmarks for 10 implementations:\n",
            "  - faiss_flat: FAISS Flat (Exact Search)\n",
            "  - faiss_ivf: FAISS IVF (nlist=100)\n",
            "  - faiss_hnsw: FAISS HNSW (M=16)\n",
            "  - faiss_pq: FAISS PQ (m=8)\n",
            "  - chroma_default: ChromaDB Default (HNSW)\n",
            "  - chroma_optimized: ChromaDB Optimized (M=32, ef=200)\n",
            "  - chroma_fast: ChromaDB Fast (Default params, large batches)\n",
            "  - annoy: Annoy (n_trees=10)\n",
            "  - qdrant_memory: Qdrant In-Memory (Default HNSW)\n",
            "  - qdrant_optimized: Qdrant Optimized (M=32, ef=200)\n",
            "\n",
            "🔄 Benchmarking faiss_flat...\n",
            "✅ faiss_flat completed successfully\n",
            "\n",
            "🔄 Benchmarking faiss_ivf...\n",
            "✅ faiss_ivf completed successfully\n",
            "\n",
            "🔄 Benchmarking faiss_hnsw...\n",
            "✅ faiss_hnsw completed successfully\n",
            "\n",
            "🔄 Benchmarking faiss_pq...\n",
            "✅ faiss_pq completed successfully\n",
            "\n",
            "🔄 Benchmarking chroma_default...\n",
            "✅ chroma_default completed successfully\n",
            "\n",
            "🔄 Benchmarking chroma_optimized...\n",
            "✅ chroma_optimized completed successfully\n",
            "\n",
            "🔄 Benchmarking chroma_fast...\n",
            "✅ chroma_fast completed successfully\n",
            "\n",
            "🔄 Benchmarking annoy...\n",
            "✅ annoy completed successfully\n",
            "\n",
            "🔄 Benchmarking qdrant_memory...\n",
            "✅ qdrant_memory completed successfully\n",
            "\n",
            "🔄 Benchmarking qdrant_optimized...\n",
            "✅ qdrant_optimized completed successfully\n",
            "\n",
            "✅ Benchmarking completed! Tested 10 implementations\n",
            "\n",
            "============================================================\n",
            "📊 ANALYZING RESULTS\n",
            "============================================================\n",
            "📋 Benchmark Results Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          algorithm      type                                    description  \\\n",
              "0        faiss_flat     FAISS                      FAISS Flat (Exact Search)   \n",
              "1         faiss_ivf     FAISS                          FAISS IVF (nlist=100)   \n",
              "2        faiss_hnsw     FAISS                              FAISS HNSW (M=16)   \n",
              "3          faiss_pq     FAISS                                 FAISS PQ (m=8)   \n",
              "4    chroma_default  ChromaDB                        ChromaDB Default (HNSW)   \n",
              "5  chroma_optimized  ChromaDB              ChromaDB Optimized (M=32, ef=200)   \n",
              "6       chroma_fast  ChromaDB  ChromaDB Fast (Default params, large batches)   \n",
              "7             annoy     Annoy                             Annoy (n_trees=10)   \n",
              "8     qdrant_memory    Qdrant                Qdrant In-Memory (Default HNSW)   \n",
              "9  qdrant_optimized    Qdrant                Qdrant Optimized (M=32, ef=200)   \n",
              "\n",
              "   build_time  memory_usage_mb  search_time_k1  queries_per_second_k1  \\\n",
              "0    0.003491        14.648438        0.009592           10425.552435   \n",
              "1    0.155080         8.789062        0.006490           15408.339150   \n",
              "2    0.214565        10.986328        0.001975           50645.651036   \n",
              "3    1.206399         2.197266        0.003419           29246.262551   \n",
              "4    6.076719         9.521484        0.160010             624.959931   \n",
              "5    5.707817        10.986328        0.179370             557.507305   \n",
              "6    4.586432         8.056641        0.176447             566.743972   \n",
              "7    0.284027         5.859375        0.009653           10359.205045   \n",
              "8    2.350519        10.253906        1.041922              95.976517   \n",
              "9    2.340325        11.718750        0.990599             100.948990   \n",
              "\n",
              "   recall@1  search_time_k5  queries_per_second_k5  recall@5  search_time_k10  \\\n",
              "0      1.00        0.009271           10786.524251     1.000         0.009413   \n",
              "1      1.00        0.004721           21181.214019     1.000         0.004887   \n",
              "2      0.99        0.001804           55424.005638     1.000         0.003158   \n",
              "3      0.48        0.003237           30895.749748     0.546         0.003508   \n",
              "4      0.99        0.179171             558.126513     1.000         0.207346   \n",
              "5      1.00        0.192057             520.677966     1.000         0.224105   \n",
              "6      1.00        0.188096             531.642611     1.000         0.204784   \n",
              "7      1.00        0.009603           10413.128429     0.978         0.009896   \n",
              "8      0.99        0.975951             102.464157     1.000         1.585995   \n",
              "9      0.99        1.025558              97.507894     1.000         1.614097   \n",
              "\n",
              "   queries_per_second_k10  recall@10  search_time_k20  queries_per_second_k20  \\\n",
              "0            10623.332151      1.000         0.009968            10032.300038   \n",
              "1            20463.346886      0.998         0.006097            16402.581049   \n",
              "2            31667.074368      1.000         0.001957            51106.421348   \n",
              "3            28505.532146      0.676         0.003821            26174.048342   \n",
              "4              482.284996      1.000         0.223231              447.966780   \n",
              "5              446.219959      1.000         0.228715              437.225781   \n",
              "6              488.318462      1.000         0.226305              441.881982   \n",
              "7            10104.646419      0.953         0.010050             9950.427026   \n",
              "8               63.051919      1.000         1.016380               98.388360   \n",
              "9               61.954153      1.000         1.007666               99.239245   \n",
              "\n",
              "   recall@20  \n",
              "0     1.0000  \n",
              "1     0.9955  \n",
              "2     0.9885  \n",
              "3     0.7595  \n",
              "4     1.0000  \n",
              "5     1.0000  \n",
              "6     1.0000  \n",
              "7     0.9275  \n",
              "8     1.0000  \n",
              "9     1.0000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-45249785-aeee-4ea2-b80f-efd77f0b2900\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>algorithm</th>\n",
              "      <th>type</th>\n",
              "      <th>description</th>\n",
              "      <th>build_time</th>\n",
              "      <th>memory_usage_mb</th>\n",
              "      <th>search_time_k1</th>\n",
              "      <th>queries_per_second_k1</th>\n",
              "      <th>recall@1</th>\n",
              "      <th>search_time_k5</th>\n",
              "      <th>queries_per_second_k5</th>\n",
              "      <th>recall@5</th>\n",
              "      <th>search_time_k10</th>\n",
              "      <th>queries_per_second_k10</th>\n",
              "      <th>recall@10</th>\n",
              "      <th>search_time_k20</th>\n",
              "      <th>queries_per_second_k20</th>\n",
              "      <th>recall@20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>faiss_flat</td>\n",
              "      <td>FAISS</td>\n",
              "      <td>FAISS Flat (Exact Search)</td>\n",
              "      <td>0.003491</td>\n",
              "      <td>14.648438</td>\n",
              "      <td>0.009592</td>\n",
              "      <td>10425.552435</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.009271</td>\n",
              "      <td>10786.524251</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.009413</td>\n",
              "      <td>10623.332151</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.009968</td>\n",
              "      <td>10032.300038</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>faiss_ivf</td>\n",
              "      <td>FAISS</td>\n",
              "      <td>FAISS IVF (nlist=100)</td>\n",
              "      <td>0.155080</td>\n",
              "      <td>8.789062</td>\n",
              "      <td>0.006490</td>\n",
              "      <td>15408.339150</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.004721</td>\n",
              "      <td>21181.214019</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.004887</td>\n",
              "      <td>20463.346886</td>\n",
              "      <td>0.998</td>\n",
              "      <td>0.006097</td>\n",
              "      <td>16402.581049</td>\n",
              "      <td>0.9955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>faiss_hnsw</td>\n",
              "      <td>FAISS</td>\n",
              "      <td>FAISS HNSW (M=16)</td>\n",
              "      <td>0.214565</td>\n",
              "      <td>10.986328</td>\n",
              "      <td>0.001975</td>\n",
              "      <td>50645.651036</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.001804</td>\n",
              "      <td>55424.005638</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.003158</td>\n",
              "      <td>31667.074368</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.001957</td>\n",
              "      <td>51106.421348</td>\n",
              "      <td>0.9885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>faiss_pq</td>\n",
              "      <td>FAISS</td>\n",
              "      <td>FAISS PQ (m=8)</td>\n",
              "      <td>1.206399</td>\n",
              "      <td>2.197266</td>\n",
              "      <td>0.003419</td>\n",
              "      <td>29246.262551</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.003237</td>\n",
              "      <td>30895.749748</td>\n",
              "      <td>0.546</td>\n",
              "      <td>0.003508</td>\n",
              "      <td>28505.532146</td>\n",
              "      <td>0.676</td>\n",
              "      <td>0.003821</td>\n",
              "      <td>26174.048342</td>\n",
              "      <td>0.7595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>chroma_default</td>\n",
              "      <td>ChromaDB</td>\n",
              "      <td>ChromaDB Default (HNSW)</td>\n",
              "      <td>6.076719</td>\n",
              "      <td>9.521484</td>\n",
              "      <td>0.160010</td>\n",
              "      <td>624.959931</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.179171</td>\n",
              "      <td>558.126513</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.207346</td>\n",
              "      <td>482.284996</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.223231</td>\n",
              "      <td>447.966780</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>chroma_optimized</td>\n",
              "      <td>ChromaDB</td>\n",
              "      <td>ChromaDB Optimized (M=32, ef=200)</td>\n",
              "      <td>5.707817</td>\n",
              "      <td>10.986328</td>\n",
              "      <td>0.179370</td>\n",
              "      <td>557.507305</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.192057</td>\n",
              "      <td>520.677966</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.224105</td>\n",
              "      <td>446.219959</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.228715</td>\n",
              "      <td>437.225781</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>chroma_fast</td>\n",
              "      <td>ChromaDB</td>\n",
              "      <td>ChromaDB Fast (Default params, large batches)</td>\n",
              "      <td>4.586432</td>\n",
              "      <td>8.056641</td>\n",
              "      <td>0.176447</td>\n",
              "      <td>566.743972</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.188096</td>\n",
              "      <td>531.642611</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.204784</td>\n",
              "      <td>488.318462</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.226305</td>\n",
              "      <td>441.881982</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>annoy</td>\n",
              "      <td>Annoy</td>\n",
              "      <td>Annoy (n_trees=10)</td>\n",
              "      <td>0.284027</td>\n",
              "      <td>5.859375</td>\n",
              "      <td>0.009653</td>\n",
              "      <td>10359.205045</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.009603</td>\n",
              "      <td>10413.128429</td>\n",
              "      <td>0.978</td>\n",
              "      <td>0.009896</td>\n",
              "      <td>10104.646419</td>\n",
              "      <td>0.953</td>\n",
              "      <td>0.010050</td>\n",
              "      <td>9950.427026</td>\n",
              "      <td>0.9275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>qdrant_memory</td>\n",
              "      <td>Qdrant</td>\n",
              "      <td>Qdrant In-Memory (Default HNSW)</td>\n",
              "      <td>2.350519</td>\n",
              "      <td>10.253906</td>\n",
              "      <td>1.041922</td>\n",
              "      <td>95.976517</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.975951</td>\n",
              "      <td>102.464157</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.585995</td>\n",
              "      <td>63.051919</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.016380</td>\n",
              "      <td>98.388360</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>qdrant_optimized</td>\n",
              "      <td>Qdrant</td>\n",
              "      <td>Qdrant Optimized (M=32, ef=200)</td>\n",
              "      <td>2.340325</td>\n",
              "      <td>11.718750</td>\n",
              "      <td>0.990599</td>\n",
              "      <td>100.948990</td>\n",
              "      <td>0.99</td>\n",
              "      <td>1.025558</td>\n",
              "      <td>97.507894</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.614097</td>\n",
              "      <td>61.954153</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.007666</td>\n",
              "      <td>99.239245</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45249785-aeee-4ea2-b80f-efd77f0b2900')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-45249785-aeee-4ea2-b80f-efd77f0b2900 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-45249785-aeee-4ea2-b80f-efd77f0b2900');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-de9a46d1-355c-4114-b18c-e9a1b58f3d4b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-de9a46d1-355c-4114-b18c-e9a1b58f3d4b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-de9a46d1-355c-4114-b18c-e9a1b58f3d4b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_45d0182d-cc12-4c99-bf05-1280234e0a33\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_45d0182d-cc12-4c99-bf05-1280234e0a33 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"algorithm\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"qdrant_memory\",\n          \"faiss_ivf\",\n          \"chroma_optimized\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"ChromaDB\",\n          \"Qdrant\",\n          \"FAISS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Qdrant In-Memory (Default HNSW)\",\n          \"FAISS IVF (nlist=100)\",\n          \"ChromaDB Optimized (M=32, ef=200)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"build_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.3709351102894396,\n        \"min\": 0.0034906864166259766,\n        \"max\": 6.07671856880188,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          2.3505187034606934,\n          0.15507960319519043,\n          5.70781683921814\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"memory_usage_mb\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.41884056961627,\n        \"min\": 2.197265625,\n        \"max\": 14.6484375,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          10.25390625,\n          8.7890625,\n          8.056640625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"search_time_k1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.40697676688203116,\n        \"min\": 0.00197450319925944,\n        \"max\": 1.0419215361277263,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1.0419215361277263,\n          0.006489992141723633,\n          0.179369846979777\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"queries_per_second_k1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16605.403591202,\n        \"min\": 95.97651697617015,\n        \"max\": 50645.65103642584,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          95.97651697617015,\n          15408.339149921017,\n          557.5073050671358\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall@1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1631086890525592,\n        \"min\": 0.48,\n        \"max\": 1.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.99,\n          0.48\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"search_time_k5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3997633893469696,\n        \"min\": 0.0018042723337809246,\n        \"max\": 1.0255579948425293,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9759510358174642,\n          0.004721164703369141,\n          0.19205729166666666\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"queries_per_second_k5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18249.401825832785,\n        \"min\": 97.50789375432116,\n        \"max\": 55424.00563802141,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          102.46415683778564,\n          21181.21401878598,\n          520.6779661016949\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall@5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1429616887296888,\n        \"min\": 0.546,\n        \"max\": 1.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.546,\n          0.978\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"search_time_k10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6464051692938713,\n        \"min\": 0.0031578540802001953,\n        \"max\": 1.6140968004862468,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1.585994561513265,\n          0.004886786142985026,\n          0.2241047223409017\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"queries_per_second_k10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12431.121334428426,\n        \"min\": 61.95415291689754,\n        \"max\": 31667.07436768592,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          63.051918604680296,\n          20463.346885672465,\n          446.21995893456835\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10180378731221695,\n        \"min\": 0.676,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.998,\n          0.9530000000000001,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"search_time_k20\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4020024013843839,\n        \"min\": 0.0019567012786865234,\n        \"max\": 1.0163803895314534,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1.0163803895314534,\n          0.006096601486206055,\n          0.22871478398640951\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"queries_per_second_k20\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16463.33224873741,\n        \"min\": 98.38836033239438,\n        \"max\": 51106.42134763007,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          98.38836033239438,\n          16402.581048844393,\n          437.2257807608192\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall@20\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07632161482102497,\n        \"min\": 0.7594999999999998,\n        \"max\": 1.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.9954999999999999,\n          0.9275,\n          0.9885\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "📈 CREATING VISUALIZATIONS\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"42d924b9-a63d-483c-88d8-fcc336fd5897\" class=\"plotly-graph-div\" style=\"height:1200px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"42d924b9-a63d-483c-88d8-fcc336fd5897\")) {                    Plotly.newPlot(                        \"42d924b9-a63d-483c-88d8-fcc336fd5897\",                        [{\"marker\":{\"color\":\"lightblue\"},\"name\":\"Build Time\",\"x\":[\"faiss_flat\",\"faiss_ivf\",\"faiss_hnsw\",\"faiss_pq\",\"chroma_default\",\"chroma_optimized\",\"chroma_fast\",\"annoy\",\"qdrant_memory\",\"qdrant_optimized\"],\"y\":[0.0034906864166259766,0.15507960319519043,0.21456480026245117,1.2063994407653809,6.07671856880188,5.70781683921814,4.586431980133057,0.2840268611907959,2.3505187034606934,2.340325117111206],\"type\":\"bar\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"marker\":{\"color\":\"lightgreen\"},\"name\":\"Memory Usage\",\"x\":[\"faiss_flat\",\"faiss_ivf\",\"faiss_hnsw\",\"faiss_pq\",\"chroma_default\",\"chroma_optimized\",\"chroma_fast\",\"annoy\",\"qdrant_memory\",\"qdrant_optimized\"],\"y\":[14.6484375,8.7890625,10.986328125,2.197265625,9.521484375,10.986328125,8.056640625,5.859375,10.25390625,11.71875],\"type\":\"bar\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"marker\":{\"color\":\"lightcoral\"},\"name\":\"Search Time\",\"x\":[\"faiss_flat\",\"faiss_ivf\",\"faiss_hnsw\",\"faiss_pq\",\"chroma_default\",\"chroma_optimized\",\"chroma_fast\",\"annoy\",\"qdrant_memory\",\"qdrant_optimized\"],\"y\":[0.00941324234008789,0.004886786142985026,0.0031578540802001953,0.0035080909729003906,0.20734628041585287,0.2241047223409017,0.20478439331054688,0.009896437327067057,1.585994561513265,1.6140968004862468],\"type\":\"bar\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"marker\":{\"color\":\"lightyellow\"},\"name\":\"Recall@10\",\"x\":[\"faiss_flat\",\"faiss_ivf\",\"faiss_hnsw\",\"faiss_pq\",\"chroma_default\",\"chroma_optimized\",\"chroma_fast\",\"annoy\",\"qdrant_memory\",\"qdrant_optimized\"],\"y\":[1.0,0.998,1.0,0.676,1.0,1.0,1.0,0.9530000000000001,1.0,1.0],\"type\":\"bar\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"marker\":{\"color\":\"lightpink\"},\"name\":\"QPS\",\"x\":[\"faiss_flat\",\"faiss_ivf\",\"faiss_hnsw\",\"faiss_pq\",\"chroma_default\",\"chroma_optimized\",\"chroma_fast\",\"annoy\",\"qdrant_memory\",\"qdrant_optimized\"],\"y\":[10623.332151360113,20463.346885672465,31667.07436768592,28505.532146255267,482.28499589884325,446.21995893456835,488.31846208296855,10104.64641922169,63.051918604680296,61.95415291689754],\"type\":\"bar\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"},{\"marker\":{\"color\":\"purple\",\"size\":10},\"mode\":\"markers+text\",\"name\":\"Accuracy vs Speed\",\"text\":[\"faiss_flat\",\"faiss_ivf\",\"faiss_hnsw\",\"faiss_pq\",\"chroma_default\",\"chroma_optimized\",\"chroma_fast\",\"annoy\",\"qdrant_memory\",\"qdrant_optimized\"],\"textposition\":\"top center\",\"x\":[0.00941324234008789,0.004886786142985026,0.0031578540802001953,0.0035080909729003906,0.20734628041585287,0.2241047223409017,0.20478439331054688,0.009896437327067057,1.585994561513265,1.6140968004862468],\"y\":[1.0,0.998,1.0,0.676,1.0,1.0,1.0,0.9530000000000001,1.0,1.0],\"type\":\"scatter\",\"xaxis\":\"x6\",\"yaxis\":\"y6\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"Algorithm\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.7777777777777778,1.0],\"title\":{\"text\":\"Time (seconds)\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"Algorithm\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.7777777777777778,1.0],\"title\":{\"text\":\"Memory (MB)\"}},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"Algorithm\"}},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.3888888888888889,0.6111111111111112],\"title\":{\"text\":\"Time (seconds)\"}},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"Algorithm\"}},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.3888888888888889,0.6111111111111112],\"title\":{\"text\":\"Recall\"}},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"Algorithm\"}},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.0,0.22222222222222224],\"title\":{\"text\":\"Queries\\u002fSecond\"}},\"xaxis6\":{\"anchor\":\"y6\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"Search Time (seconds)\"}},\"yaxis6\":{\"anchor\":\"x6\",\"domain\":[0.0,0.22222222222222224],\"title\":{\"text\":\"Recall@10\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Build Time Comparison\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Memory Usage Comparison\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Search Time (k=10)\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6111111111111112,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Recall@10 Accuracy\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6111111111111112,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Queries Per Second (k=10)\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.22222222222222224,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Accuracy vs Speed Trade-off\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.22222222222222224,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Vector Database Indexing Performance Comparison\"},\"height\":1200,\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('42d924b9-a63d-483c-88d8-fcc336fd5897');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🔍 DETAILED PERFORMANCE ANALYSIS\n",
            "============================================================\n",
            "🏆 Performance Rankings:\n",
            "========================================\n",
            "\n",
            "📊 Build Time:\n",
            "  1. faiss_flat: 0.0035 s\n",
            "  2. faiss_ivf: 0.1551 s\n",
            "  3. faiss_hnsw: 0.2146 s\n",
            "  4. annoy: 0.2840 s\n",
            "  5. faiss_pq: 1.2064 s\n",
            "  6. qdrant_optimized: 2.3403 s\n",
            "  7. qdrant_memory: 2.3505 s\n",
            "  8. chroma_fast: 4.5864 s\n",
            "  9. chroma_optimized: 5.7078 s\n",
            "  10. chroma_default: 6.0767 s\n",
            "\n",
            "📊 Memory Usage Mb:\n",
            "  1. faiss_pq: 2.1973 MB\n",
            "  2. annoy: 5.8594 MB\n",
            "  3. chroma_fast: 8.0566 MB\n",
            "  4. faiss_ivf: 8.7891 MB\n",
            "  5. chroma_default: 9.5215 MB\n",
            "  6. qdrant_memory: 10.2539 MB\n",
            "  7. chroma_optimized: 10.9863 MB\n",
            "  8. faiss_hnsw: 10.9863 MB\n",
            "  9. qdrant_optimized: 11.7188 MB\n",
            "  10. faiss_flat: 14.6484 MB\n",
            "\n",
            "📊 Search Time K10:\n",
            "  1. faiss_hnsw: 0.0032 s\n",
            "  2. faiss_pq: 0.0035 s\n",
            "  3. faiss_ivf: 0.0049 s\n",
            "  4. faiss_flat: 0.0094 s\n",
            "  5. annoy: 0.0099 s\n",
            "  6. chroma_fast: 0.2048 s\n",
            "  7. chroma_default: 0.2073 s\n",
            "  8. chroma_optimized: 0.2241 s\n",
            "  9. qdrant_memory: 1.5860 s\n",
            "  10. qdrant_optimized: 1.6141 s\n",
            "\n",
            "📊 Recall@10:\n",
            "  1. faiss_flat: 1.0000 \n",
            "  2. faiss_hnsw: 1.0000 \n",
            "  3. qdrant_optimized: 1.0000 \n",
            "  4. chroma_default: 1.0000 \n",
            "  5. chroma_optimized: 1.0000 \n",
            "  6. chroma_fast: 1.0000 \n",
            "  7. qdrant_memory: 1.0000 \n",
            "  8. faiss_ivf: 0.9980 \n",
            "  9. annoy: 0.9530 \n",
            "  10. faiss_pq: 0.6760 \n",
            "\n",
            "📊 Queries Per Second K10:\n",
            "  1. faiss_hnsw: 31667.0744 QPS\n",
            "  2. faiss_pq: 28505.5321 QPS\n",
            "  3. faiss_ivf: 20463.3469 QPS\n",
            "  4. faiss_flat: 10623.3322 QPS\n",
            "  5. annoy: 10104.6464 QPS\n",
            "  6. chroma_fast: 488.3185 QPS\n",
            "  7. chroma_default: 482.2850 QPS\n",
            "  8. chroma_optimized: 446.2200 QPS\n",
            "  9. qdrant_memory: 63.0519 QPS\n",
            "  10. qdrant_optimized: 61.9542 QPS\n",
            "\n",
            "🎯 Efficiency Analysis:\n",
            "========================================\n",
            "🏆 Most Efficient: faiss_hnsw\n",
            "   Efficiency Score: 316.57\n",
            "   Recall@10: 1.0000\n",
            "   Search Time: 0.0032s\n",
            "\n",
            "💾 Most Memory Efficient:\n",
            "  1. faiss_pq: 2.2 MB\n",
            "  2. annoy: 5.9 MB\n",
            "  3. chroma_fast: 8.1 MB\n",
            "\n",
            "⚡ Fastest Search:\n",
            "  1. faiss_hnsw: 31667.1 QPS\n",
            "  2. faiss_pq: 28505.5 QPS\n",
            "  3. faiss_ivf: 20463.3 QPS\n",
            "\n",
            "🎯 Most Accurate:\n",
            "  1. faiss_flat: 1.0000 recall\n",
            "  2. faiss_hnsw: 1.0000 recall\n",
            "  3. chroma_default: 1.0000 recall\n",
            "\n",
            "============================================================\n",
            "💡 RECOMMENDATIONS\n",
            "============================================================\n",
            "🏆 Best Overall Performance: faiss_hnsw\n",
            "🏭 Best for Production: faiss_hnsw\n",
            "💾 Best for Large Scale: faiss_pq\n",
            "⚡ Best for Real-time: faiss_hnsw\n",
            "\n",
            "🎯 Use Case Guidelines (Based on Available Implementations):\n",
            "  📊 High Accuracy Required: ✅ FAISS Flat or HNSW variants\n",
            "  ⚡ Speed Critical: ✅ FAISS IVF or Annoy\n",
            "  💾 Memory Constrained: ✅ FAISS PQ\n",
            "  🔄 Frequent Updates: ✅ ChromaDB or Qdrant\n",
            "  📈 Balanced Workload: ✅ HNSW-based implementations (FAISS HNSW, ChromaDB)\n",
            "  🏭 Production Scale: ✅ Qdrant local/cloud\n",
            "  🔬 Research/Experimental: ✅ FAISS variants (comprehensive testing)\n",
            "\n",
            "============================================================\n",
            "💾 EXPORTING RESULTS\n",
            "============================================================\n",
            "✅ Results exported successfully!\n",
            "📁 Files created:\n",
            "  - vector_db_benchmark_results.csv\n",
            "  - vector_db_benchmark_summary.json\n",
            "\n",
            "✨ Vector Database Indexing Evaluation Completed! ✨\n",
            "\n",
            "📊 Benchmark Summary:\n",
            "  • Successfully tested: 10 vector database implementations\n",
            "  • Available databases: 4/6\n",
            "  • Skipped due to installation issues: NMSLIB, ScaNN\n",
            "    (This is normal in Colab environments)\n",
            "\n",
            "🚀 Next Steps:\n",
            "  1. Fine-tune parameters for your best performing algorithm\n",
            "  2. Test with larger datasets to validate scalability\n",
            "  3. Consider setting up Qdrant local server for production testing\n",
            "  4. Implement the chosen solution in your production environment\n",
            "  5. Monitor performance metrics in real-world usage\n",
            "\n",
            "💡 Package Installation Notes:\n",
            "  • NMSLIB: Often fails in Colab due to compilation requirements\n",
            "  • ScaNN: May have TensorFlow version conflicts in Colab\n",
            "  • For production use, install these packages in a dedicated environment\n",
            "  • The core results from FAISS and ChromaDB provide excellent benchmarks\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install vector database packages with error handling\n",
        "print(\"📦 Installing vector database packages...\")\n",
        "\n",
        "# Install Annoy\n",
        "print(\"Installing Annoy...\")\n",
        "try:\n",
        "    !pip install -q annoy\n",
        "    annoy_available = True\n",
        "    print(\"✅ Annoy installed successfully\")\n",
        "except Exception as e:\n",
        "    annoy_available = False\n",
        "    print(f\"⚠️ Annoy installation failed: {e}\")\n",
        "\n",
        "# Install NMSLIB (try different package names)\n",
        "print(\"Installing NMSLIB...\")\n",
        "nmslib_available = False\n",
        "try:\n",
        "    !pip install -q nmslib\n",
        "    nmslib_available = True\n",
        "    print(\"✅ NMSLIB installed successfully\")\n",
        "except:\n",
        "    print(\"⚠️ NMSLIB installation failed - this is common in some environments\")\n",
        "\n",
        "# Try to install Qdrant\n",
        "print(\"Installing Qdrant...\")\n",
        "try:\n",
        "    !pip install -q qdrant-client\n",
        "    qdrant_available = True\n",
        "    print(\"✅ Qdrant installed successfully\")\n",
        "except Exception as e:\n",
        "    qdrant_available = False\n",
        "    print(f\"⚠️ Qdrant installation failed: {e}\")\n",
        "\n",
        "# Try to install ScaNN\n",
        "print(\"Installing ScaNN...\")\n",
        "try:\n",
        "    !pip install -q scann\n",
        "    scann_available = True\n",
        "    print(\"✅ ScaNN installed successfully\")\n",
        "except Exception as e:\n",
        "    scann_available = False\n",
        "    print(f\"⚠️ ScaNN installation failed: {e}\")\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import subprocess\n",
        "\n",
        "# Vector database and search libraries\n",
        "import faiss\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# Conditional imports based on availability\n",
        "if annoy_available:\n",
        "    try:\n",
        "        from annoy import AnnoyIndex\n",
        "        print(\"✅ Annoy imported successfully\")\n",
        "    except ImportError as e:\n",
        "        annoy_available = False\n",
        "        print(f\"⚠️ Annoy import failed: {e}\")\n",
        "\n",
        "if nmslib_available:\n",
        "    try:\n",
        "        import nmslib\n",
        "        print(\"✅ NMSLIB imported successfully\")\n",
        "    except ImportError as e:\n",
        "        nmslib_available = False\n",
        "        print(f\"⚠️ NMSLIB import failed: {e}\")\n",
        "\n",
        "if qdrant_available:\n",
        "    try:\n",
        "        from qdrant_client import QdrantClient\n",
        "        from qdrant_client.http import models\n",
        "        from qdrant_client.http.models import Distance, VectorParams\n",
        "        print(\"✅ Qdrant imported successfully\")\n",
        "    except ImportError as e:\n",
        "        qdrant_available = False\n",
        "        print(f\"⚠️ Qdrant import failed: {e}\")\n",
        "\n",
        "if scann_available:\n",
        "    try:\n",
        "        import scann\n",
        "        print(\"✅ ScaNN imported successfully\")\n",
        "    except (ImportError, Exception) as e:\n",
        "        scann_available = False\n",
        "        print(f\"⚠️ ScaNN import failed: {e}\")\n",
        "        print(\"   This is often due to TensorFlow compatibility issues in Colab\")\n",
        "\n",
        "# Final status check\n",
        "print(\"\\n🔧 Performing final import verification...\")\n",
        "final_status = {\n",
        "    'FAISS': True,\n",
        "    'ChromaDB': True,\n",
        "    'Annoy': annoy_available,\n",
        "    'NMSLIB': nmslib_available,\n",
        "    'Qdrant': qdrant_available,\n",
        "    'ScaNN': scann_available\n",
        "}\n",
        "\n",
        "# Update availability flags based on successful imports\n",
        "if not annoy_available:\n",
        "    print(\"❌ Annoy: Import failed - will be skipped\")\n",
        "if not nmslib_available:\n",
        "    print(\"❌ NMSLIB: Compilation/import failed - will be skipped (common in Colab)\")\n",
        "if not qdrant_available:\n",
        "    print(\"❌ Qdrant: Import failed - will be skipped\")\n",
        "if not scann_available:\n",
        "    print(\"❌ ScaNN: TensorFlow compatibility issue - will be skipped (common in Colab)\")\n",
        "\n",
        "# Additional imports\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import time\n",
        "import psutil\n",
        "import gc\n",
        "from memory_profiler import memory_usage\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"\\n📊 Final Vector Database Availability:\")\n",
        "print(f\"  ✅ FAISS: Available (4 variants) - Core library\")\n",
        "print(f\"  ✅ ChromaDB: Available (3 variants) - Core library\")\n",
        "print(f\"  {'✅' if annoy_available else '❌'} Annoy: {'Available' if annoy_available else 'Failed - Compilation issue'}\")\n",
        "print(f\"  {'✅' if nmslib_available else '❌'} NMSLIB: {'Available' if nmslib_available else 'Failed - Common in Colab environments'}\")\n",
        "print(f\"  {'✅' if qdrant_available else '❌'} Qdrant: {'Available (4 variants)' if qdrant_available else 'Failed - Installation issue'}\")\n",
        "print(f\"  {'✅' if scann_available else '❌'} ScaNN: {'Available' if scann_available else 'Failed - TensorFlow compatibility'}\")\n",
        "\n",
        "total_variants = 7  # FAISS (4) + ChromaDB (3)\n",
        "if annoy_available:\n",
        "    total_variants += 1\n",
        "if nmslib_available:\n",
        "    total_variants += 1\n",
        "if qdrant_available:\n",
        "    total_variants += 4\n",
        "if scann_available:\n",
        "    total_variants += 1\n",
        "\n",
        "print(f\"\\n🎯 Total vector database variants to test: {total_variants}\")\n",
        "\n",
        "if total_variants < 12:\n",
        "    print(\"\\n💡 Note: Some packages failed to install/import. This is normal in Colab.\")\n",
        "    print(\"   The notebook will test all available implementations and still provide\")\n",
        "    print(\"   comprehensive benchmarking results with the working vector databases.\")\n",
        "\n",
        "print(f\"\\n🚀 Proceeding with {sum(final_status.values())} available vector database types...\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 3: DATA LOADING & PREPARATION\n",
        "# ==========================================\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload your StarTech dataset (CSV file):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the filename\n",
        "filename = list(uploaded.keys())[0]\n",
        "print(f\"📁 Loaded file: {filename}\")\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(filename)\n",
        "\n",
        "print(f\"📊 Dataset shape: {df.shape}\")\n",
        "print(f\"🏷️  Columns: {list(df.columns)}\")\n",
        "\n",
        "# Prepare text data for vectorization\n",
        "if 'text' in df.columns:\n",
        "    # Use existing text column (from chunked data)\n",
        "    documents = df['text'].tolist()\n",
        "    print(\"📄 Using existing text column\")\n",
        "else:\n",
        "    # Create text from product data\n",
        "    def create_text_representation(row):\n",
        "        text_parts = []\n",
        "        if pd.notna(row['name']):\n",
        "            text_parts.append(f\"{row['name']}\")\n",
        "        if pd.notna(row['brand']):\n",
        "            text_parts.append(f\"Brand: {row['brand']}\")\n",
        "        if pd.notna(row['category']):\n",
        "            text_parts.append(f\"Category: {row['category']}\")\n",
        "        if pd.notna(row['subcategory']):\n",
        "            text_parts.append(f\"Subcategory: {row['subcategory']}\")\n",
        "        return \". \".join(text_parts)\n",
        "\n",
        "    documents = df.apply(create_text_representation, axis=1).tolist()\n",
        "    print(\"📄 Created text representations from product data\")\n",
        "\n",
        "# Limit dataset size for testing (you can adjust this)\n",
        "MAX_DOCUMENTS = 5000\n",
        "if len(documents) > MAX_DOCUMENTS:\n",
        "    print(f\"📉 Limiting dataset to {MAX_DOCUMENTS} documents for testing\")\n",
        "    documents = documents[:MAX_DOCUMENTS]\n",
        "    df = df.head(MAX_DOCUMENTS)\n",
        "\n",
        "print(f\"📝 Prepared {len(documents)} documents for vector database testing\")\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 4: GENERATE EMBEDDINGS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🧠 GENERATING EMBEDDINGS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize embedding model\n",
        "print(\"📥 Loading embedding model...\")\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"✅ Embedding model loaded successfully!\")\n",
        "\n",
        "# Generate embeddings\n",
        "print(\"🔄 Generating embeddings for all documents...\")\n",
        "start_time = time.time()\n",
        "embeddings = embedding_model.encode(documents, show_progress_bar=True, convert_to_numpy=True)\n",
        "embedding_time = time.time() - start_time\n",
        "\n",
        "print(f\"✅ Generated {len(embeddings)} embeddings in {embedding_time:.2f} seconds\")\n",
        "print(f\"📊 Embedding shape: {embeddings.shape}\")\n",
        "print(f\"📏 Embedding dimension: {embeddings.shape[1]}\")\n",
        "\n",
        "# Normalize embeddings for cosine similarity\n",
        "embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Create test queries (subset of embeddings for accuracy testing)\n",
        "test_size = min(100, len(embeddings) // 10)\n",
        "test_indices = np.random.choice(len(embeddings), test_size, replace=False)\n",
        "test_queries = embeddings_normalized[test_indices]\n",
        "print(f\"🎯 Created {len(test_queries)} test queries for accuracy evaluation\")\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 4: SYSTEM INFO & BENCHMARKING SETUP\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"💻 SYSTEM INFORMATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get system information\n",
        "def get_system_info():\n",
        "    info = {\n",
        "        'cpu_count': psutil.cpu_count(),\n",
        "        'memory_total': psutil.virtual_memory().total / (1024**3),  # GB\n",
        "        'memory_available': psutil.virtual_memory().available / (1024**3),  # GB\n",
        "    }\n",
        "    return info\n",
        "\n",
        "sys_info = get_system_info()\n",
        "print(f\"🖥️  CPU Cores: {sys_info['cpu_count']}\")\n",
        "print(f\"💾 Total Memory: {sys_info['memory_total']:.1f} GB\")\n",
        "print(f\"💽 Available Memory: {sys_info['memory_available']:.1f} GB\")\n",
        "\n",
        "# Benchmarking parameters\n",
        "SEARCH_K_VALUES = [1, 5, 10, 20]\n",
        "BUILD_METRICS = ['build_time', 'index_size_mb', 'memory_usage_mb']\n",
        "SEARCH_METRICS = ['search_time', 'recall@1', 'recall@5', 'recall@10', 'queries_per_second']\n",
        "\n",
        "print(f\"🎯 Testing with k values: {SEARCH_K_VALUES}\")\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 5: GROUND TRUTH COMPUTATION\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎯 COMPUTING GROUND TRUTH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"🔄 Computing exact nearest neighbors for accuracy evaluation...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Use sklearn for exact nearest neighbors\n",
        "exact_nn = NearestNeighbors(n_neighbors=max(SEARCH_K_VALUES), metric='cosine', algorithm='brute')\n",
        "exact_nn.fit(embeddings_normalized)\n",
        "distances, ground_truth_indices = exact_nn.kneighbors(test_queries)\n",
        "\n",
        "gt_time = time.time() - start_time\n",
        "print(f\"✅ Ground truth computed in {gt_time:.2f} seconds\")\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 6: UTILITY FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def measure_memory_usage(func, *args, **kwargs):\n",
        "    \"\"\"Measure memory usage of a function\"\"\"\n",
        "    mem_usage = memory_usage((func, args, kwargs), interval=0.1, timeout=60)\n",
        "    return max(mem_usage) - min(mem_usage)\n",
        "\n",
        "def calculate_recall(predicted_indices, ground_truth_indices, k):\n",
        "    \"\"\"Calculate recall@k\"\"\"\n",
        "    if len(predicted_indices) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    recalls = []\n",
        "    for pred, gt in zip(predicted_indices, ground_truth_indices):\n",
        "        pred_k = pred[:k] if len(pred) >= k else pred\n",
        "        gt_k = gt[:k] if len(gt) >= k else gt\n",
        "\n",
        "        intersection = len(set(pred_k) & set(gt_k))\n",
        "        recalls.append(intersection / len(gt_k))\n",
        "\n",
        "    return np.mean(recalls)\n",
        "\n",
        "def benchmark_search(search_func, test_queries, k_values, num_trials=3):\n",
        "    \"\"\"Benchmark search performance\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for k in k_values:\n",
        "        times = []\n",
        "        all_indices = []\n",
        "\n",
        "        for trial in range(num_trials):\n",
        "            start_time = time.time()\n",
        "            indices = search_func(test_queries, k)\n",
        "            search_time = time.time() - start_time\n",
        "            times.append(search_time)\n",
        "            if trial == 0:  # Use first trial for accuracy\n",
        "                all_indices = indices\n",
        "\n",
        "        avg_time = np.mean(times)\n",
        "        qps = len(test_queries) / avg_time\n",
        "\n",
        "        results[f'search_time_k{k}'] = avg_time\n",
        "        results[f'queries_per_second_k{k}'] = qps\n",
        "        results[f'recall@{k}'] = calculate_recall(all_indices, ground_truth_indices, k)\n",
        "\n",
        "    return results\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 7: FAISS IMPLEMENTATIONS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🚀 TESTING FAISS IMPLEMENTATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class FAISSBenchmark:\n",
        "    def __init__(self, embeddings, dimension):\n",
        "        self.embeddings = embeddings.astype('float32')\n",
        "        self.dimension = dimension\n",
        "        self.indexes = {}\n",
        "\n",
        "    def build_flat_index(self):\n",
        "        \"\"\"Build FAISS Flat (exact) index\"\"\"\n",
        "        print(\"🔧 Building FAISS Flat index...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        index = faiss.IndexFlatIP(self.dimension)  # Inner product for cosine similarity\n",
        "        index.add(self.embeddings)\n",
        "\n",
        "        build_time = time.time() - start_time\n",
        "\n",
        "        # Estimate memory usage\n",
        "        memory_mb = (self.embeddings.nbytes + index.ntotal * self.dimension * 4) / (1024**2)\n",
        "\n",
        "        self.indexes['faiss_flat'] = {\n",
        "            'index': index,\n",
        "            'build_time': build_time,\n",
        "            'memory_usage_mb': memory_mb,\n",
        "            'description': 'FAISS Flat (Exact Search)'\n",
        "        }\n",
        "        print(f\"✅ FAISS Flat built in {build_time:.2f}s, Memory: {memory_mb:.1f}MB\")\n",
        "\n",
        "    def build_ivf_index(self, nlist=100):\n",
        "        \"\"\"Build FAISS IVF index\"\"\"\n",
        "        print(f\"🔧 Building FAISS IVF index (nlist={nlist})...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        quantizer = faiss.IndexFlatIP(self.dimension)\n",
        "        index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)\n",
        "\n",
        "        # Train the index\n",
        "        index.train(self.embeddings)\n",
        "        index.add(self.embeddings)\n",
        "\n",
        "        build_time = time.time() - start_time\n",
        "        memory_mb = self.embeddings.nbytes / (1024**2) * 1.2  # Approximate\n",
        "\n",
        "        self.indexes['faiss_ivf'] = {\n",
        "            'index': index,\n",
        "            'build_time': build_time,\n",
        "            'memory_usage_mb': memory_mb,\n",
        "            'description': f'FAISS IVF (nlist={nlist})'\n",
        "        }\n",
        "        print(f\"✅ FAISS IVF built in {build_time:.2f}s, Memory: {memory_mb:.1f}MB\")\n",
        "\n",
        "    def build_hnsw_index(self, M=16):\n",
        "        \"\"\"Build FAISS HNSW index\"\"\"\n",
        "        print(f\"🔧 Building FAISS HNSW index (M={M})...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        index = faiss.IndexHNSWFlat(self.dimension, M)\n",
        "        index.add(self.embeddings)\n",
        "\n",
        "        build_time = time.time() - start_time\n",
        "        memory_mb = self.embeddings.nbytes / (1024**2) * 1.5  # Approximate\n",
        "\n",
        "        self.indexes['faiss_hnsw'] = {\n",
        "            'index': index,\n",
        "            'build_time': build_time,\n",
        "            'memory_usage_mb': memory_mb,\n",
        "            'description': f'FAISS HNSW (M={M})'\n",
        "        }\n",
        "        print(f\"✅ FAISS HNSW built in {build_time:.2f}s, Memory: {memory_mb:.1f}MB\")\n",
        "\n",
        "    def build_pq_index(self, m=8):\n",
        "        \"\"\"Build FAISS Product Quantization index\"\"\"\n",
        "        print(f\"🔧 Building FAISS PQ index (m={m})...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        index = faiss.IndexPQ(self.dimension, m, 8)  # 8-bit quantization\n",
        "        index.train(self.embeddings)\n",
        "        index.add(self.embeddings)\n",
        "\n",
        "        build_time = time.time() - start_time\n",
        "        memory_mb = self.embeddings.nbytes / (1024**2) * 0.3  # Much smaller\n",
        "\n",
        "        self.indexes['faiss_pq'] = {\n",
        "            'index': index,\n",
        "            'build_time': build_time,\n",
        "            'memory_usage_mb': memory_mb,\n",
        "            'description': f'FAISS PQ (m={m})'\n",
        "        }\n",
        "        print(f\"✅ FAISS PQ built in {build_time:.2f}s, Memory: {memory_mb:.1f}MB\")\n",
        "\n",
        "    def search(self, index_name, queries, k):\n",
        "        \"\"\"Search using specified index\"\"\"\n",
        "        index = self.indexes[index_name]['index']\n",
        "\n",
        "        if index_name == 'faiss_ivf':\n",
        "            index.nprobe = min(10, index.nlist)  # Set nprobe for IVF\n",
        "\n",
        "        scores, indices = index.search(queries.astype('float32'), k)\n",
        "        return indices\n",
        "\n",
        "# Initialize and build FAISS indexes\n",
        "dimension = embeddings.shape[1]\n",
        "faiss_benchmark = FAISSBenchmark(embeddings_normalized, dimension)\n",
        "\n",
        "# Build all FAISS indexes\n",
        "faiss_benchmark.build_flat_index()\n",
        "faiss_benchmark.build_ivf_index()\n",
        "faiss_benchmark.build_hnsw_index()\n",
        "faiss_benchmark.build_pq_index()\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 8: CHROMADB IMPLEMENTATION\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎨 TESTING CHROMADB\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class ChromaDBBenchmark:\n",
        "    def __init__(self, embeddings, documents):\n",
        "        self.embeddings = embeddings\n",
        "        self.documents = documents\n",
        "        self.clients = {}\n",
        "\n",
        "    def build_default_index(self):\n",
        "        \"\"\"Build ChromaDB with default settings\"\"\"\n",
        "        print(\"🔧 Building ChromaDB default index...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Initialize ChromaDB client with persistent storage\n",
        "        client = chromadb.PersistentClient(path=\"./chroma_db_default\")\n",
        "\n",
        "        # Delete collection if it exists (for clean testing)\n",
        "        try:\n",
        "            client.delete_collection(\"startech_default\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Create collection with default HNSW settings\n",
        "        collection = client.create_collection(\n",
        "            name=\"startech_default\",\n",
        "            metadata={\"hnsw:space\": \"cosine\"}\n",
        "        )\n",
        "\n",
        "        # Add documents with embeddings in batches\n",
        "        batch_size = 100\n",
        "        ids = [str(i) for i in range(len(self.documents))]\n",
        "\n",
        "        for i in range(0, len(self.documents), batch_size):\n",
        "            end_idx = min(i + batch_size, len(self.documents))\n",
        "            collection.add(\n",
        "                embeddings=self.embeddings[i:end_idx].tolist(),\n",
        "                documents=self.documents[i:end_idx],\n",
        "                ids=ids[i:end_idx]\n",
        "            )\n",
        "\n",
        "        build_time = time.time() - start_time\n",
        "        memory_mb = self.embeddings.nbytes / (1024**2) * 1.3\n",
        "\n",
        "        self.clients['chroma_default'] = {\n",
        "            'client': client,\n",
        "            'collection': collection,\n",
        "            'build_time': build_time,\n",
        "            'memory_usage_mb': memory_mb,\n",
        "            'description': 'ChromaDB Default (HNSW)'\n",
        "        }\n",
        "        print(f\"✅ ChromaDB Default built in {build_time:.2f}s, Memory: {memory_mb:.1f}MB\")\n",
        "\n",
        "    def build_optimized_index(self):\n",
        "        \"\"\"Build ChromaDB with optimized HNSW settings\"\"\"\n",
        "        print(\"🔧 Building ChromaDB optimized index...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Initialize ChromaDB client with optimized settings\n",
        "            client = chromadb.PersistentClient(path=\"./chroma_db_optimized\")\n",
        "\n",
        "            # Delete collection if it exists\n",
        "            try:\n",
        "                client.delete_collection(\"startech_optimized\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # Create collection with simplified optimized HNSW parameters\n",
        "            # Using only well-supported parameters\n",
        "            collection = client.create_collection(\n",
        "                name=\"startech_optimized\",\n",
        "                metadata={\n",
        "                    \"hnsw:space\": \"cosine\",\n",
        "                    \"hnsw:construction_ef\": 200,  # Use construction_ef instead of ef_construction\n",
        "                    \"hnsw:M\": 32  # Keep M parameter simple\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # Add documents with embeddings in batches\n",
        "            batch_size = 100\n",
        "            ids = [str(i) for i in range(len(self.documents))]\n",
        "\n",
        "            for i in range(0, len(self.documents), batch_size):\n",
        "                end_idx = min(i + batch_size, len(self.documents))\n",
        "                collection.add(\n",
        "                    embeddings=self.embeddings[i:end_idx].tolist(),\n",
        "                    documents=self.documents[i:end_idx],\n",
        "                    ids=ids[i:end_idx]\n",
        "                )\n",
        "\n",
        "            build_time = time.time() - start_time\n",
        "            memory_mb = self.embeddings.nbytes / (1024**2) * 1.5  # Higher memory for optimized settings\n",
        "\n",
        "            self.clients['chroma_optimized'] = {\n",
        "                'client': client,\n",
        "                'collection': collection,\n",
        "                'build_time': build_time,\n",
        "                'memory_usage_mb': memory_mb,\n",
        "                'description': 'ChromaDB Optimized (M=32, ef=200)'\n",
        "            }\n",
        "            print(f\"✅ ChromaDB Optimized built in {build_time:.2f}s, Memory: {memory_mb:.1f}MB\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  ChromaDB Optimized build failed: {e}\")\n",
        "            print(\"   Trying fallback configuration...\")\n",
        "\n",
        "            # Fallback: Try with minimal optimizations\n",
        "            try:\n",
        "                client = chromadb.PersistentClient(path=\"./chroma_db_optimized_fallback\")\n",
        "\n",
        "                try:\n",
        "                    client.delete_collection(\"startech_optimized_fallback\")\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                # Create collection with minimal metadata\n",
        "                collection = client.create_collection(\n",
        "                    name=\"startech_optimized_fallback\",\n",
        "                    metadata={\"hnsw:space\": \"cosine\"}  # Only essential parameter\n",
        "                )\n",
        "\n",
        "                # Add documents\n",
        "                batch_size = 100\n",
        "                ids = [str(i) for i in range(len(self.documents))]\n",
        "\n",
        "                for i in range(0, len(self.documents), batch_size):\n",
        "                    end_idx = min(i + batch_size, len(self.documents))\n",
        "                    collection.add(\n",
        "                        embeddings=self.embeddings[i:end_idx].tolist(),\n",
        "                        documents=self.documents[i:end_idx],\n",
        "                        ids=ids[i:end_idx]\n",
        "                    )\n",
        "\n",
        "                build_time = time.time() - start_time\n",
        "                memory_mb = self.embeddings.nbytes / (1024**2) * 1.3\n",
        "\n",
        "                self.clients['chroma_optimized'] = {\n",
        "                    'client': client,\n",
        "                    'collection': collection,\n",
        "                    'build_time': build_time,\n",
        "                    'memory_usage_mb': memory_mb,\n",
        "                    'description': 'ChromaDB Optimized (Fallback)'\n",
        "                }\n",
        "                print(f\"✅ ChromaDB Optimized (fallback) built in {build_time:.2f}s, Memory: {memory_mb:.1f}MB\")\n",
        "\n",
        "            except Exception as e2:\n",
        "                print(f\"❌ ChromaDB Optimized completely failed: {e2}\")\n",
        "                print(\"   Skipping optimized configuration\")\n",
        "\n",
        "    def build_fast_index(self):\n",
        "        \"\"\"Build ChromaDB with speed-optimized settings\"\"\"\n",
        "        print(\"🔧 Building ChromaDB fast index...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Initialize ChromaDB client with speed-optimized settings\n",
        "            client = chromadb.PersistentClient(path=\"./chroma_db_fast\")\n",
        "\n",
        "            # Delete collection if it exists\n",
        "            try:\n",
        "                client.delete_collection(\"startech_fast\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # Create collection with simple fast configuration\n",
        "            collection = client.create_collection(\n",
        "                name=\"startech_fast\",\n",
        "                metadata={\n",
        "                    \"hnsw:space\": \"cosine\"\n",
        "                    # Remove problematic parameters, use defaults for speed\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # Add documents with embeddings in larger batches for speed\n",
        "            batch_size = 200  # Larger batches for faster insertion\n",
        "            ids = [str(i) for i in range(len(self.documents))]\n",
        "\n",
        "            for i in range(0, len(self.documents), batch_size):\n",
        "                end_idx = min(i + batch_size, len(self.documents))\n",
        "                collection.add(\n",
        "                    embeddings=self.embeddings[i:end_idx].tolist(),\n",
        "                    documents=self.documents[i:end_idx],\n",
        "                    ids=ids[i:end_idx]\n",
        "                )\n",
        "\n",
        "            build_time = time.time() - start_time\n",
        "            memory_mb = self.embeddings.nbytes / (1024**2) * 1.1  # Lower memory for fast settings\n",
        "\n",
        "            self.clients['chroma_fast'] = {\n",
        "                'client': client,\n",
        "                'collection': collection,\n",
        "                'build_time': build_time,\n",
        "                'memory_usage_mb': memory_mb,\n",
        "                'description': 'ChromaDB Fast (Default params, large batches)'\n",
        "            }\n",
        "            print(f\"✅ ChromaDB Fast built in {build_time:.2f}s, Memory: {memory_mb:.1f}MB\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ ChromaDB Fast build failed: {e}\")\n",
        "            print(\"   Skipping fast configuration\")\n",
        "\n",
        "    def search(self, client_name, queries, k):\n",
        "        \"\"\"Search using ChromaDB\"\"\"\n",
        "        collection = self.clients[client_name]['collection']\n",
        "\n",
        "        all_indices = []\n",
        "        for query in queries:\n",
        "            results = collection.query(\n",
        "                query_embeddings=[query.tolist()],\n",
        "                n_results=k\n",
        "            )\n",
        "            # Convert string IDs back to integers\n",
        "            indices = [int(id_) for id_ in results['ids'][0]]\n",
        "            all_indices.append(indices)\n",
        "\n",
        "        return all_indices\n",
        "\n",
        "# Initialize and build ChromaDB variants\n",
        "chroma_benchmark = ChromaDBBenchmark(embeddings_normalized, documents)\n",
        "chroma_benchmark.build_default_index()\n",
        "chroma_benchmark.build_optimized_index()\n",
        "chroma_benchmark.build_fast_index()\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 9: ANNOY IMPLEMENTATION\n",
        "# ==========================================\n",
        "\n",
        "if annoy_available:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🎪 TESTING ANNOY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    class AnnoyBenchmark:\n",
        "        def __init__(self, embeddings, dimension):\n",
        "            self.embeddings = embeddings\n",
        "            self.dimension = dimension\n",
        "            self.indexes = {}\n",
        "\n",
        "        def build_index(self, n_trees=10):\n",
        "            \"\"\"Build Annoy index\"\"\"\n",
        "            print(f\"🔧 Building Annoy index (n_trees={n_trees})...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            index = AnnoyIndex(self.dimension, 'angular')  # Angular for cosine similarity\n",
        "\n",
        "            for i, embedding in enumerate(self.embeddings):\n",
        "                index.add_item(i, embedding)\n",
        "\n",
        "            index.build(n_trees)\n",
        "\n",
        "            build_time = time.time() - start_time\n",
        "            memory_mb = self.embeddings.nbytes / (1024**2) * 0.8  # Approximate\n",
        "\n",
        "            self.indexes['annoy'] = {\n",
        "                'index': index,\n",
        "                'build_time': build_time,\n",
        "                'memory_usage_mb': memory_mb,\n",
        "                'description': f'Annoy (n_trees={n_trees})'\n",
        "            }\n",
        "            print(f\"✅ Annoy built in {build_time:.2f}s, Memory: {memory_mb:.1f}MB\")\n",
        "\n",
        "        def search(self, queries, k):\n",
        "            \"\"\"Search using Annoy\"\"\"\n",
        "            index = self.indexes['annoy']['index']\n",
        "\n",
        "            all_indices = []\n",
        "            for query in queries:\n",
        "                indices, _ = index.get_nns_by_vector(query, k, include_distances=True)\n",
        "                all_indices.append(indices)\n",
        "\n",
        "            return all_indices\n",
        "\n",
        "    # Initialize and build Annoy\n",
        "    annoy_benchmark = AnnoyBenchmark(embeddings_normalized, dimension)\n",
        "    annoy_benchmark.build_index()\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"⚠️  ANNOY NOT AVAILABLE\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Annoy is not available. Skipping Annoy benchmarks.\")\n",
        "    annoy_benchmark = None\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 10: NMSLIB IMPLEMENTATION\n",
        "# ==========================================\n",
        "\n",
        "if nmslib_available:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"📚 TESTING NMSLIB\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    class NMSLIBBenchmark:\n",
        "        def __init__(self, embeddings):\n",
        "            self.embeddings = embeddings\n",
        "            self.indexes = {}\n",
        "\n",
        "        def build_hnsw_index(self, M=16, efC=200):\n",
        "            \"\"\"Build NMSLIB HNSW index\"\"\"\n",
        "            print(f\"🔧 Building NMSLIB HNSW index (M={M}, efC={efC})...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            index = nmslib.init(method='hnsw', space='cosinesimil')\n",
        "\n",
        "            for i, embedding in enumerate(self.embeddings):\n",
        "                index.addDataPoint(i, embedding)\n",
        "\n",
        "            index.createIndex({'M': M, 'indexThreadQty': 1, 'efConstruction': efC})\n",
        "\n",
        "            build_time = time.time() - start_time\n",
        "            memory_mb = self.embeddings.nbytes / (1024**2) * 1.2  # Approximate\n",
        "\n",
        "            self.indexes['nmslib_hnsw'] = {\n",
        "                'index': index,\n",
        "                'build_time': build_time,\n",
        "                'memory_usage_mb': memory_mb,\n",
        "                'description': f'NMSLIB HNSW (M={M}, efC={efC})'\n",
        "            }\n",
        "            print(f\"✅ NMSLIB HNSW built in {build_time:.2f}s, Memory: {memory_mb:.1f}MB\")\n",
        "\n",
        "        def search(self, queries, k, ef=50):\n",
        "            \"\"\"Search using NMSLIB\"\"\"\n",
        "            index = self.indexes['nmslib_hnsw']['index']\n",
        "            index.setQueryTimeParams({'efSearch': ef})\n",
        "\n",
        "            all_indices = []\n",
        "            for query in queries:\n",
        "                indices, _ = index.knnQuery(query, k=k)\n",
        "                all_indices.append(indices)\n",
        "\n",
        "            return all_indices\n",
        "\n",
        "    # Initialize and build NMSLIB\n",
        "    nmslib_benchmark = NMSLIBBenchmark(embeddings_normalized)\n",
        "    nmslib_benchmark.build_hnsw_index()\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"⚠️  NMSLIB NOT AVAILABLE\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"NMSLIB is not available. Skipping NMSLIB benchmarks.\")\n",
        "    nmslib_benchmark = None\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 10.5: SCANN IMPLEMENTATION (OPTIONAL)\n",
        "# ==========================================\n",
        "\n",
        "if scann_available:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🔬 TESTING SCANN\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    class ScaNNBenchmark:\n",
        "        def __init__(self, embeddings):\n",
        "            self.embeddings = embeddings.astype('float32')\n",
        "            self.indexes = {}\n",
        "\n",
        "        def build_index(self, num_leaves=100, num_leaves_to_search=10):\n",
        "            \"\"\"Build ScaNN index\"\"\"\n",
        "            print(f\"🔧 Building ScaNN index (leaves={num_leaves})...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            try:\n",
        "                # Create ScaNN searcher\n",
        "                searcher = (scann.scann_ops_pybind.builder(self.embeddings, 10, \"dot_product\")\n",
        "                           .tree(num_leaves=num_leaves, num_leaves_to_search=num_leaves_to_search,\n",
        "                                training_sample_size=min(len(self.embeddings), 5000))\n",
        "                           .score_ah(2, anisotropic_quantization_threshold=0.2)\n",
        "                           .reorder(100)\n",
        "                           .build())\n",
        "\n",
        "                build_time = time.time() - start_time\n",
        "                memory_mb = self.embeddings.nbytes / (1024**2) * 0.9  # ScaNN is memory efficient\n",
        "\n",
        "                self.indexes['scann'] = {\n",
        "                    'searcher': searcher,\n",
        "                    'build_time': build_time,\n",
        "                    'memory_usage_mb': memory_mb,\n",
        "                    'description': f'ScaNN (leaves={num_leaves})'\n",
        "                }\n",
        "                print(f\"✅ ScaNN built in {build_time:.2f}s, Memory: {memory_mb:.1f}MB\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ ScaNN build failed: {e}\")\n",
        "\n",
        "        def search(self, queries, k):\n",
        "            \"\"\"Search using ScaNN\"\"\"\n",
        "            if 'scann' not in self.indexes:\n",
        "                return []\n",
        "\n",
        "            searcher = self.indexes['scann']['searcher']\n",
        "\n",
        "            all_indices = []\n",
        "            for query in queries:\n",
        "                try:\n",
        "                    indices, _ = searcher.search(query.astype('float32'), final_num_neighbors=k)\n",
        "                    all_indices.append(indices.tolist())\n",
        "                except:\n",
        "                    all_indices.append([])\n",
        "\n",
        "            return all_indices\n",
        "\n",
        "    # Initialize and build ScaNN\n",
        "    scann_benchmark = ScaNNBenchmark(embeddings_normalized)\n",
        "    scann_benchmark.build_index()\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"⚠️  SCANN NOT AVAILABLE\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"ScaNN is not available. Skipping ScaNN benchmarks.\")\n",
        "    scann_benchmark = None\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 11.5: QDRANT IMPLEMENTATION (OPTIONAL)\n",
        "# ==========================================\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 11: QDRANT IMPLEMENTATION (OPTIONAL)\n",
        "# ==========================================\n",
        "\n",
        "if qdrant_available:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🎯 TESTING QDRANT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Check for Qdrant configuration options\n",
        "    print(\"🔧 Qdrant Setup Options:\")\n",
        "    print(\"  1. In-memory (for testing)\")\n",
        "    print(\"  2. Local server (requires Docker)\")\n",
        "    print(\"  3. Qdrant Cloud (requires API key)\")\n",
        "    print()\n",
        "\n",
        "    # You can uncomment and configure these options:\n",
        "\n",
        "    # Option 1: Docker setup (uncomment to use)\n",
        "    # print(\"🐳 To use local Qdrant server, run this command first:\")\n",
        "    # print(\"docker run -p 6333:6333 qdrant/qdrant\")\n",
        "    # print()\n",
        "\n",
        "    # Option 2: Cloud setup (uncomment and add your API key)\n",
        "    QDRANT_CLOUD_URL = \"https://840547c8-bb6a-48a2-90ca-7f9943af6976.europe-west3-0.gcp.cloud.qdrant.io:6333\"\n",
        "    QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.QPAdSIP0myaxTG33YwopAR8kgWjdOHLYbPMV1XILdrk\"\n",
        "\n",
        "\n",
        "    class QdrantBenchmark:\n",
        "        def __init__(self, embeddings, documents):\n",
        "            self.embeddings = embeddings\n",
        "            self.documents = documents\n",
        "            self.clients = {}\n",
        "\n",
        "        def build_memory_index(self):\n",
        "            \"\"\"Build Qdrant in-memory index (for testing)\"\"\"\n",
        "            print(\"🔧 Building Qdrant in-memory index...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            try:\n",
        "                # In-memory client for testing\n",
        "                client = QdrantClient(\":memory:\")\n",
        "\n",
        "                # Create collection with default HNSW settings\n",
        "                client.create_collection(\n",
        "                    collection_name=\"startech_memory\",\n",
        "                    vectors_config=VectorParams(\n",
        "                        size=self.embeddings.shape[1],\n",
        "                        distance=Distance.COSINE\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                # Prepare and add points in batches\n",
        "                batch_size = 100\n",
        "                for i in range(0, len(self.embeddings), batch_size):\n",
        "                    end_idx = min(i + batch_size, len(self.embeddings))\n",
        "                    points = [\n",
        "                        models.PointStruct(\n",
        "                            id=idx,\n",
        "                            vector=self.embeddings[idx].tolist(),\n",
        "                            payload={\"text\": self.documents[idx]}\n",
        "                        )\n",
        "                        for idx in range(i, end_idx)\n",
        "                    ]\n",
        "                    client.upsert(collection_name=\"startech_memory\", points=points)\n",
        "\n",
        "                build_time = time.time() - start_time\n",
        "                memory_mb = self.embeddings.nbytes / (1024**2) * 1.4\n",
        "\n",
        "                self.clients['qdrant_memory'] = {\n",
        "                    'client': client,\n",
        "                    'collection_name': 'startech_memory',\n",
        "                    'build_time': build_time,\n",
        "                    'memory_usage_mb': memory_mb,\n",
        "                    'description': 'Qdrant In-Memory (Default HNSW)'\n",
        "                }\n",
        "                print(f\"✅ Qdrant Memory built in {build_time:.2f}s, Memory: {memory_mb:.1f}MB\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Qdrant Memory build failed: {e}\")\n",
        "\n",
        "        def build_optimized_index(self):\n",
        "            \"\"\"Build Qdrant with optimized HNSW settings\"\"\"\n",
        "            print(\"🔧 Building Qdrant optimized index...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            try:\n",
        "                # In-memory client with optimized settings\n",
        "                client = QdrantClient(\":memory:\")\n",
        "\n",
        "                # Create collection with optimized HNSW parameters\n",
        "                client.create_collection(\n",
        "                    collection_name=\"startech_optimized\",\n",
        "                    vectors_config=VectorParams(\n",
        "                        size=self.embeddings.shape[1],\n",
        "                        distance=Distance.COSINE,\n",
        "                        hnsw_config=models.HnswConfigDiff(\n",
        "                            m=32,  # Higher M for better accuracy\n",
        "                            ef_construct=200,  # Higher ef_construct for better index quality\n",
        "                            full_scan_threshold=1000,\n",
        "                            max_indexing_threads=4\n",
        "                        )\n",
        "                    ),\n",
        "                    optimizers_config=models.OptimizersConfigDiff(\n",
        "                        default_segment_number=1,\n",
        "                        max_segment_size=50000\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                # Add points in batches\n",
        "                batch_size = 100\n",
        "                for i in range(0, len(self.embeddings), batch_size):\n",
        "                    end_idx = min(i + batch_size, len(self.embeddings))\n",
        "                    points = [\n",
        "                        models.PointStruct(\n",
        "                            id=idx,\n",
        "                            vector=self.embeddings[idx].tolist(),\n",
        "                            payload={\"text\": self.documents[idx]}\n",
        "                        )\n",
        "                        for idx in range(i, end_idx)\n",
        "                    ]\n",
        "                    client.upsert(collection_name=\"startech_optimized\", points=points)\n",
        "\n",
        "                build_time = time.time() - start_time\n",
        "                memory_mb = self.embeddings.nbytes / (1024**2) * 1.6  # Higher for optimized\n",
        "\n",
        "                self.clients['qdrant_optimized'] = {\n",
        "                    'client': client,\n",
        "                    'collection_name': 'startech_optimized',\n",
        "                    'build_time': build_time,\n",
        "                    'memory_usage_mb': memory_mb,\n",
        "                    'description': 'Qdrant Optimized (M=32, ef=200)'\n",
        "                }\n",
        "                print(f\"✅ Qdrant Optimized built in {build_time:.2f}s, Memory: {memory_mb:.1f}MB\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Qdrant Optimized build failed: {e}\")\n",
        "\n",
        "        def build_local_server_index(self):\n",
        "            \"\"\"Build Qdrant using local server (requires Docker)\"\"\"\n",
        "            print(\"🔧 Attempting to connect to local Qdrant server...\")\n",
        "\n",
        "            try:\n",
        "                # Try to connect to local Qdrant server\n",
        "                client = QdrantClient(host=\"localhost\", port=6333)\n",
        "\n",
        "                # Test connection\n",
        "                client.get_collections()\n",
        "                print(\"✅ Connected to local Qdrant server\")\n",
        "\n",
        "                start_time = time.time()\n",
        "\n",
        "                # Delete collection if it exists\n",
        "                try:\n",
        "                    client.delete_collection(\"startech_local\")\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                # Create collection\n",
        "                client.create_collection(\n",
        "                    collection_name=\"startech_local\",\n",
        "                    vectors_config=VectorParams(\n",
        "                        size=self.embeddings.shape[1],\n",
        "                        distance=Distance.COSINE,\n",
        "                        hnsw_config=models.HnswConfigDiff(\n",
        "                            m=16,\n",
        "                            ef_construct=100,\n",
        "                            max_indexing_threads=4\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                # Add points in batches\n",
        "                batch_size = 100\n",
        "                for i in range(0, len(self.embeddings), batch_size):\n",
        "                    end_idx = min(i + batch_size, len(self.embeddings))\n",
        "                    points = [\n",
        "                        models.PointStruct(\n",
        "                            id=idx,\n",
        "                            vector=self.embeddings[idx].tolist(),\n",
        "                            payload={\"text\": self.documents[idx]}\n",
        "                        )\n",
        "                        for idx in range(i, end_idx)\n",
        "                    ]\n",
        "                    client.upsert(collection_name=\"startech_local\", points=points)\n",
        "\n",
        "                build_time = time.time() - start_time\n",
        "                memory_mb = self.embeddings.nbytes / (1024**2) * 1.3\n",
        "\n",
        "                self.clients['qdrant_local'] = {\n",
        "                    'client': client,\n",
        "                    'collection_name': 'startech_local',\n",
        "                    'build_time': build_time,\n",
        "                    'memory_usage_mb': memory_mb,\n",
        "                    'description': 'Qdrant Local Server'\n",
        "                }\n",
        "                print(f\"✅ Qdrant Local built in {build_time:.2f}s, Memory: {memory_mb:.1f}MB\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  Local Qdrant server not available: {e}\")\n",
        "                print(\"💡 To use local server, run: docker run -p 6333:6333 qdrant/qdrant\")\n",
        "\n",
        "        def build_cloud_index(self):\n",
        "            \"\"\"Build Qdrant using cloud service (requires API key)\"\"\"\n",
        "            # Uncomment and configure these if you have Qdrant Cloud access\n",
        "            QDRANT_CLOUD_URL = \"https://840547c8-bb6a-48a2-90ca-7f9943af6976.europe-west3-0.gcp.cloud.qdrant.io:6333\"\n",
        "            QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.QPAdSIP0myaxTG33YwopAR8kgWjdOHLYbPMV1XILdrk\"\n",
        "\n",
        "\n",
        "            if 'QDRANT_CLOUD_URL' in locals() and 'QDRANT_API_KEY' in locals():\n",
        "                print(\"🔧 Building Qdrant cloud index...\")\n",
        "                try:\n",
        "                    client = QdrantClient(\n",
        "                        url=QDRANT_CLOUD_URL,\n",
        "                        api_key=QDRANT_API_KEY\n",
        "                    )\n",
        "                    # ... rest of cloud implementation\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Qdrant Cloud build failed: {e}\")\n",
        "            else:\n",
        "              print(\"☁️  Qdrant Cloud setup not configured\")\n",
        "              print(\"💡 To use Qdrant Cloud:\")\n",
        "              print(\"   1. Sign up at https://cloud.qdrant.io/\")\n",
        "              print(\"   2. Create a cluster\")\n",
        "              print(\"   3. Uncomment and configure cloud settings in the code\")\n",
        "\n",
        "        def search(self, client_name, queries, k, ef=None):\n",
        "            \"\"\"Search using Qdrant\"\"\"\n",
        "            client_data = self.clients[client_name]\n",
        "            client = client_data['client']\n",
        "            collection_name = client_data['collection_name']\n",
        "\n",
        "            # Set search parameters if specified\n",
        "            search_params = None\n",
        "            if ef:\n",
        "                search_params = models.SearchParams(\n",
        "                    hnsw_ef=ef,\n",
        "                    exact=False\n",
        "                )\n",
        "\n",
        "            all_indices = []\n",
        "            for query in queries:\n",
        "                results = client.search(\n",
        "                    collection_name=collection_name,\n",
        "                    query_vector=query.tolist(),\n",
        "                    limit=k,\n",
        "                    search_params=search_params\n",
        "                )\n",
        "                indices = [hit.id for hit in results]\n",
        "                all_indices.append(indices)\n",
        "\n",
        "            return all_indices\n",
        "\n",
        "    # Initialize and build Qdrant variants\n",
        "    qdrant_benchmark = QdrantBenchmark(embeddings_normalized, documents)\n",
        "    qdrant_benchmark.build_memory_index()\n",
        "    qdrant_benchmark.build_optimized_index()\n",
        "    qdrant_benchmark.build_local_server_index()  # Will skip if Docker not running\n",
        "    qdrant_benchmark.build_cloud_index()  # Will show setup instructions\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 12: COMPREHENSIVE BENCHMARKING\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🏃‍♂️ RUNNING COMPREHENSIVE BENCHMARKS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Collect all benchmarks\n",
        "benchmarks = {}\n",
        "\n",
        "# Add FAISS benchmarks\n",
        "for name, data in faiss_benchmark.indexes.items():\n",
        "    benchmarks[name] = {\n",
        "        'type': 'FAISS',\n",
        "        'description': data['description'],\n",
        "        'build_time': data['build_time'],\n",
        "        'memory_usage_mb': data['memory_usage_mb'],\n",
        "        'search_func': lambda q, k, n=name: faiss_benchmark.search(n, q, k)\n",
        "    }\n",
        "\n",
        "# Add ChromaDB benchmarks (multiple variants)\n",
        "for name, data in chroma_benchmark.clients.items():\n",
        "    benchmarks[name] = {\n",
        "        'type': 'ChromaDB',\n",
        "        'description': data['description'],\n",
        "        'build_time': data['build_time'],\n",
        "        'memory_usage_mb': data['memory_usage_mb'],\n",
        "        'search_func': lambda q, k, n=name: chroma_benchmark.search(n, q, k)\n",
        "    }\n",
        "\n",
        "# Add Annoy benchmark if available\n",
        "if annoy_available and annoy_benchmark is not None:\n",
        "    for name, data in annoy_benchmark.indexes.items():\n",
        "        benchmarks[name] = {\n",
        "            'type': 'Annoy',\n",
        "            'description': data['description'],\n",
        "            'build_time': data['build_time'],\n",
        "            'memory_usage_mb': data['memory_usage_mb'],\n",
        "            'search_func': lambda q, k: annoy_benchmark.search(q, k)\n",
        "        }\n",
        "\n",
        "# Add NMSLIB benchmark if available\n",
        "if nmslib_available and nmslib_benchmark is not None:\n",
        "    for name, data in nmslib_benchmark.indexes.items():\n",
        "        benchmarks[name] = {\n",
        "            'type': 'NMSLIB',\n",
        "            'description': data['description'],\n",
        "            'build_time': data['build_time'],\n",
        "            'memory_usage_mb': data['memory_usage_mb'],\n",
        "            'search_func': lambda q, k: nmslib_benchmark.search(q, k)\n",
        "        }\n",
        "\n",
        "# Add ScaNN benchmark if available\n",
        "if scann_available and scann_benchmark is not None:\n",
        "    for name, data in scann_benchmark.indexes.items():\n",
        "        benchmarks[name] = {\n",
        "            'type': 'ScaNN',\n",
        "            'description': data['description'],\n",
        "            'build_time': data['build_time'],\n",
        "            'memory_usage_mb': data['memory_usage_mb'],\n",
        "            'search_func': lambda q, k: scann_benchmark.search(q, k)\n",
        "        }\n",
        "\n",
        "# Add Qdrant benchmarks if available (multiple variants)\n",
        "if qdrant_available and 'qdrant_benchmark' in globals() and hasattr(qdrant_benchmark, 'clients') and qdrant_benchmark.clients:\n",
        "    for name, data in qdrant_benchmark.clients.items():\n",
        "        benchmarks[name] = {\n",
        "            'type': 'Qdrant',\n",
        "            'description': data['description'],\n",
        "            'build_time': data['build_time'],\n",
        "            'memory_usage_mb': data['memory_usage_mb'],\n",
        "            'search_func': lambda q, k, n=name: qdrant_benchmark.search(n, q, k)\n",
        "        }\n",
        "\n",
        "print(f\"🎯 Running benchmarks for {len(benchmarks)} implementations:\")\n",
        "for name, data in benchmarks.items():\n",
        "    print(f\"  - {name}: {data['description']}\")\n",
        "\n",
        "# Run search benchmarks\n",
        "results = []\n",
        "\n",
        "for name, benchmark in benchmarks.items():\n",
        "    print(f\"\\n🔄 Benchmarking {name}...\")\n",
        "\n",
        "    try:\n",
        "        search_results = benchmark_search(\n",
        "            benchmark['search_func'],\n",
        "            test_queries,\n",
        "            SEARCH_K_VALUES\n",
        "        )\n",
        "\n",
        "        # Combine build and search metrics\n",
        "        result = {\n",
        "            'algorithm': name,\n",
        "            'type': benchmark['type'],\n",
        "            'description': benchmark['description'],\n",
        "            'build_time': benchmark['build_time'],\n",
        "            'memory_usage_mb': benchmark['memory_usage_mb'],\n",
        "            **search_results\n",
        "        }\n",
        "\n",
        "        results.append(result)\n",
        "        print(f\"✅ {name} completed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ {name} failed: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n✅ Benchmarking completed! Tested {len(results)} implementations\")\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 13: RESULTS ANALYSIS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📊 ANALYZING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if not results:\n",
        "    print(\"❌ No benchmark results available!\")\n",
        "    exit()\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"📋 Benchmark Results Summary:\")\n",
        "display(results_df)\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 14: VISUALIZATION\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📈 CREATING VISUALIZATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create comprehensive comparison plots\n",
        "fig = make_subplots(\n",
        "    rows=3, cols=2,\n",
        "    subplot_titles=(\n",
        "        'Build Time Comparison', 'Memory Usage Comparison',\n",
        "        'Search Time (k=10)', 'Recall@10 Accuracy',\n",
        "        'Queries Per Second (k=10)', 'Accuracy vs Speed Trade-off'\n",
        "    ),\n",
        "    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
        "           [{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
        "           [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
        ")\n",
        "\n",
        "# 1. Build Time\n",
        "fig.add_trace(\n",
        "    go.Bar(x=results_df['algorithm'], y=results_df['build_time'],\n",
        "           name=\"Build Time\", marker_color='lightblue'),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# 2. Memory Usage\n",
        "fig.add_trace(\n",
        "    go.Bar(x=results_df['algorithm'], y=results_df['memory_usage_mb'],\n",
        "           name=\"Memory Usage\", marker_color='lightgreen'),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# 3. Search Time (k=10)\n",
        "if 'search_time_k10' in results_df.columns:\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=results_df['algorithm'], y=results_df['search_time_k10'],\n",
        "               name=\"Search Time\", marker_color='lightcoral'),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "# 4. Recall@10\n",
        "if 'recall@10' in results_df.columns:\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=results_df['algorithm'], y=results_df['recall@10'],\n",
        "               name=\"Recall@10\", marker_color='lightyellow'),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "# 5. Queries Per Second (k=10)\n",
        "if 'queries_per_second_k10' in results_df.columns:\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=results_df['algorithm'], y=results_df['queries_per_second_k10'],\n",
        "               name=\"QPS\", marker_color='lightpink'),\n",
        "        row=3, col=1\n",
        "    )\n",
        "\n",
        "# 6. Accuracy vs Speed Trade-off\n",
        "if 'recall@10' in results_df.columns and 'search_time_k10' in results_df.columns:\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=results_df['search_time_k10'],\n",
        "            y=results_df['recall@10'],\n",
        "            mode='markers+text',\n",
        "            text=results_df['algorithm'],\n",
        "            textposition=\"top center\",\n",
        "            marker=dict(size=10, color='purple'),\n",
        "            name=\"Accuracy vs Speed\"\n",
        "        ),\n",
        "        row=3, col=2\n",
        "    )\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    height=1200,\n",
        "    title_text=\"Vector Database Indexing Performance Comparison\",\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "# Update axis labels\n",
        "fig.update_xaxes(title_text=\"Algorithm\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Time (seconds)\", row=1, col=1)\n",
        "fig.update_xaxes(title_text=\"Algorithm\", row=1, col=2)\n",
        "fig.update_yaxes(title_text=\"Memory (MB)\", row=1, col=2)\n",
        "fig.update_xaxes(title_text=\"Algorithm\", row=2, col=1)\n",
        "fig.update_yaxes(title_text=\"Time (seconds)\", row=2, col=1)\n",
        "fig.update_xaxes(title_text=\"Algorithm\", row=2, col=2)\n",
        "fig.update_yaxes(title_text=\"Recall\", row=2, col=2)\n",
        "fig.update_xaxes(title_text=\"Algorithm\", row=3, col=1)\n",
        "fig.update_yaxes(title_text=\"Queries/Second\", row=3, col=1)\n",
        "fig.update_xaxes(title_text=\"Search Time (seconds)\", row=3, col=2)\n",
        "fig.update_yaxes(title_text=\"Recall@10\", row=3, col=2)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 15: DETAILED PERFORMANCE ANALYSIS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🔍 DETAILED PERFORMANCE ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Performance rankings\n",
        "metrics_to_rank = ['build_time', 'memory_usage_mb', 'search_time_k10', 'recall@10', 'queries_per_second_k10']\n",
        "\n",
        "print(\"🏆 Performance Rankings:\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "for metric in metrics_to_rank:\n",
        "    if metric in results_df.columns:\n",
        "        ascending = metric != 'recall@10' and metric != 'queries_per_second_k10'  # Higher is better for these\n",
        "        ranking = results_df.nsmallest(len(results_df), metric) if ascending else results_df.nlargest(len(results_df), metric)\n",
        "\n",
        "        print(f\"\\n📊 {metric.replace('_', ' ').title()}:\")\n",
        "        for i, (_, row) in enumerate(ranking.iterrows()):\n",
        "            value = row[metric]\n",
        "            unit = \"s\" if \"time\" in metric else \"MB\" if \"memory\" in metric else \"QPS\" if \"queries\" in metric else \"\"\n",
        "            print(f\"  {i+1}. {row['algorithm']}: {value:.4f} {unit}\")\n",
        "\n",
        "# Efficiency analysis\n",
        "print(\"\\n🎯 Efficiency Analysis:\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "if 'recall@10' in results_df.columns and 'search_time_k10' in results_df.columns:\n",
        "    # Calculate efficiency score (recall / search_time)\n",
        "    results_df['efficiency_score'] = results_df['recall@10'] / (results_df['search_time_k10'] + 1e-6)\n",
        "\n",
        "    best_efficiency = results_df.loc[results_df['efficiency_score'].idxmax()]\n",
        "    print(f\"🏆 Most Efficient: {best_efficiency['algorithm']}\")\n",
        "    print(f\"   Efficiency Score: {best_efficiency['efficiency_score']:.2f}\")\n",
        "    print(f\"   Recall@10: {best_efficiency['recall@10']:.4f}\")\n",
        "    print(f\"   Search Time: {best_efficiency['search_time_k10']:.4f}s\")\n",
        "\n",
        "# Memory efficiency\n",
        "if 'memory_usage_mb' in results_df.columns:\n",
        "    memory_ranking = results_df.nsmallest(3, 'memory_usage_mb')\n",
        "    print(f\"\\n💾 Most Memory Efficient:\")\n",
        "    for i, (_, row) in enumerate(memory_ranking.iterrows()):\n",
        "        print(f\"  {i+1}. {row['algorithm']}: {row['memory_usage_mb']:.1f} MB\")\n",
        "\n",
        "# Speed champions\n",
        "if 'queries_per_second_k10' in results_df.columns:\n",
        "    speed_ranking = results_df.nlargest(3, 'queries_per_second_k10')\n",
        "    print(f\"\\n⚡ Fastest Search:\")\n",
        "    for i, (_, row) in enumerate(speed_ranking.iterrows()):\n",
        "        print(f\"  {i+1}. {row['algorithm']}: {row['queries_per_second_k10']:.1f} QPS\")\n",
        "\n",
        "# Accuracy leaders\n",
        "if 'recall@10' in results_df.columns:\n",
        "    accuracy_ranking = results_df.nlargest(3, 'recall@10')\n",
        "    print(f\"\\n🎯 Most Accurate:\")\n",
        "    for i, (_, row) in enumerate(accuracy_ranking.iterrows()):\n",
        "        print(f\"  {i+1}. {row['algorithm']}: {row['recall@10']:.4f} recall\")\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 16: RECOMMENDATIONS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"💡 RECOMMENDATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def get_recommendations(results_df):\n",
        "    recommendations = []\n",
        "\n",
        "    # Best overall (balanced performance)\n",
        "    if 'efficiency_score' in results_df.columns:\n",
        "        best_overall = results_df.loc[results_df['efficiency_score'].idxmax(), 'algorithm']\n",
        "        recommendations.append(f\"🏆 Best Overall Performance: {best_overall}\")\n",
        "\n",
        "    # Best for production (accuracy + reasonable speed)\n",
        "    if 'recall@10' in results_df.columns and 'search_time_k10' in results_df.columns:\n",
        "        high_accuracy = results_df[results_df['recall@10'] > 0.9]\n",
        "        if not high_accuracy.empty:\n",
        "            production_choice = high_accuracy.loc[high_accuracy['search_time_k10'].idxmin(), 'algorithm']\n",
        "            recommendations.append(f\"🏭 Best for Production: {production_choice}\")\n",
        "\n",
        "    # Best for large scale (memory efficient)\n",
        "    if 'memory_usage_mb' in results_df.columns:\n",
        "        memory_efficient = results_df.loc[results_df['memory_usage_mb'].idxmin(), 'algorithm']\n",
        "        recommendations.append(f\"💾 Best for Large Scale: {memory_efficient}\")\n",
        "\n",
        "    # Best for real-time (fastest search)\n",
        "    if 'queries_per_second_k10' in results_df.columns:\n",
        "        fastest = results_df.loc[results_df['queries_per_second_k10'].idxmax(), 'algorithm']\n",
        "        recommendations.append(f\"⚡ Best for Real-time: {fastest}\")\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "recommendations = get_recommendations(results_df)\n",
        "for rec in recommendations:\n",
        "    print(rec)\n",
        "\n",
        "print(\"\\n🎯 Use Case Guidelines (Based on Available Implementations):\")\n",
        "if final_status['FAISS']:\n",
        "    print(\"  📊 High Accuracy Required: ✅ FAISS Flat or HNSW variants\")\n",
        "else:\n",
        "    print(\"  📊 High Accuracy Required: ❌ FAISS not available\")\n",
        "\n",
        "speed_options = []\n",
        "if final_status['FAISS']:\n",
        "    speed_options.append(\"FAISS IVF\")\n",
        "if annoy_available:\n",
        "    speed_options.append(\"Annoy\")\n",
        "if speed_options:\n",
        "    print(f\"  ⚡ Speed Critical: ✅ {' or '.join(speed_options)}\")\n",
        "else:\n",
        "    print(\"  ⚡ Speed Critical: ⚠️  Limited options available\")\n",
        "\n",
        "memory_options = []\n",
        "if final_status['FAISS']:\n",
        "    memory_options.append(\"FAISS PQ\")\n",
        "if scann_available:\n",
        "    memory_options.append(\"ScaNN\")\n",
        "if memory_options:\n",
        "    print(f\"  💾 Memory Constrained: ✅ {' or '.join(memory_options)}\")\n",
        "else:\n",
        "    print(\"  💾 Memory Constrained: ✅ FAISS PQ (if available)\")\n",
        "\n",
        "update_options = []\n",
        "if final_status['ChromaDB']:\n",
        "    update_options.append(\"ChromaDB\")\n",
        "if qdrant_available:\n",
        "    update_options.append(\"Qdrant\")\n",
        "if update_options:\n",
        "    print(f\"  🔄 Frequent Updates: ✅ {' or '.join(update_options)}\")\n",
        "else:\n",
        "    print(\"  🔄 Frequent Updates: ⚠️  Limited options available\")\n",
        "\n",
        "print(\"  📈 Balanced Workload: ✅ HNSW-based implementations (FAISS HNSW, ChromaDB)\")\n",
        "\n",
        "if qdrant_available:\n",
        "    print(\"  🏭 Production Scale: ✅ Qdrant local/cloud\")\n",
        "else:\n",
        "    print(\"  🏭 Production Scale: ✅ ChromaDB persistent or FAISS\")\n",
        "\n",
        "research_options = []\n",
        "if scann_available:\n",
        "    research_options.append(\"ScaNN\")\n",
        "if nmslib_available:\n",
        "    research_options.append(\"NMSLIB\")\n",
        "if research_options:\n",
        "    print(f\"  🔬 Research/Experimental: ✅ {' or '.join(research_options)}\")\n",
        "else:\n",
        "    print(\"  🔬 Research/Experimental: ✅ FAISS variants (comprehensive testing)\")\n",
        "\n",
        "# ==========================================\n",
        "# SECTION 17: EXPORT RESULTS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"💾 EXPORTING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Export detailed results\n",
        "results_df.to_csv('vector_db_benchmark_results.csv', index=False)\n",
        "\n",
        "# Create summary report\n",
        "summary_data = {\n",
        "    'system_info': sys_info,\n",
        "    'dataset_info': {\n",
        "        'num_documents': len(documents),\n",
        "        'embedding_dimension': dimension,\n",
        "        'test_queries': len(test_queries)\n",
        "    },\n",
        "    'recommendations': recommendations,\n",
        "    'top_performers': {}\n",
        "}\n",
        "\n",
        "# Add top performers for each metric\n",
        "for metric in ['build_time', 'memory_usage_mb', 'search_time_k10', 'recall@10', 'queries_per_second_k10']:\n",
        "    if metric in results_df.columns:\n",
        "        ascending = metric not in ['recall@10', 'queries_per_second_k10']\n",
        "        best = results_df.nsmallest(1, metric) if ascending else results_df.nlargest(1, metric)\n",
        "        summary_data['top_performers'][metric] = {\n",
        "            'algorithm': best.iloc[0]['algorithm'],\n",
        "            'value': float(best.iloc[0][metric])\n",
        "        }\n",
        "\n",
        "# Export summary\n",
        "with open('vector_db_benchmark_summary.json', 'w') as f:\n",
        "    json.dump(summary_data, f, indent=2)\n",
        "\n",
        "print(\"✅ Results exported successfully!\")\n",
        "print(\"📁 Files created:\")\n",
        "print(\"  - vector_db_benchmark_results.csv\")\n",
        "print(\"  - vector_db_benchmark_summary.json\")\n",
        "\n",
        "print(\"\\n✨ Vector Database Indexing Evaluation Completed! ✨\")\n",
        "\n",
        "print(f\"\\n📊 Benchmark Summary:\")\n",
        "print(f\"  • Successfully tested: {len(results)} vector database implementations\")\n",
        "print(f\"  • Available databases: {sum(final_status.values())}/{len(final_status)}\")\n",
        "\n",
        "if not all(final_status.values()):\n",
        "    failed_dbs = [db for db, status in final_status.items() if not status]\n",
        "    print(f\"  • Skipped due to installation issues: {', '.join(failed_dbs)}\")\n",
        "    print(f\"    (This is normal in Colab environments)\")\n",
        "\n",
        "print(\"\\n🚀 Next Steps:\")\n",
        "print(\"  1. Fine-tune parameters for your best performing algorithm\")\n",
        "print(\"  2. Test with larger datasets to validate scalability\")\n",
        "\n",
        "if qdrant_available:\n",
        "    print(\"  3. Consider setting up Qdrant local server for production testing\")\n",
        "else:\n",
        "    print(\"  3. Consider ChromaDB for production use (persistent storage)\")\n",
        "\n",
        "print(\"  4. Implement the chosen solution in your production environment\")\n",
        "print(\"  5. Monitor performance metrics in real-world usage\")\n",
        "\n",
        "if not all(final_status.values()):\n",
        "    print(\"\\n💡 Package Installation Notes:\")\n",
        "    if not nmslib_available:\n",
        "        print(\"  • NMSLIB: Often fails in Colab due to compilation requirements\")\n",
        "    if not scann_available:\n",
        "        print(\"  • ScaNN: May have TensorFlow version conflicts in Colab\")\n",
        "    if not annoy_available:\n",
        "        print(\"  • Annoy: May need manual compilation in some environments\")\n",
        "    print(\"  • For production use, install these packages in a dedicated environment\")\n",
        "    print(\"  • The core results from FAISS and ChromaDB provide excellent benchmarks\")"
      ]
    }
  ]
}